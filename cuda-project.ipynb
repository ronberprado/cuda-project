{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rrNl3umjl2H"
      },
      "source": [
        "##C++ Program"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIBUQskdNTMz"
      },
      "source": [
        "####vector size: 2^20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GpgyTfGX4yE",
        "outputId": "35b7b895-f97e-4172-e7c6-dd929df48119"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing c_dotproduct.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile c_dotproduct.cpp\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "\n",
        "//C++ Function\n",
        "void dotProduct(int n, float* X, float* Y, double* dotp){\n",
        "  dotp[0] = 0.0;\n",
        "  for(int i = 0; i < n; i++){\n",
        "    dotp[i] = dotp[i-1] + (X[i] * Y[i]);\n",
        "  }\n",
        "}\n",
        "\n",
        "//logic\n",
        "double bulk_logic(unsigned int array_size){\n",
        "  const unsigned int ARRAY_SIZE = array_size;\n",
        "  const unsigned int ARRAY_BYTES = ARRAY_SIZE * sizeof(float);\n",
        "\n",
        "  //declare array\n",
        "  float *X, *Y;\n",
        "  double *dotp;\n",
        "  X = (float*)malloc(ARRAY_BYTES);\n",
        "  Y = (float*)malloc(ARRAY_BYTES);\n",
        "  dotp = (double*)malloc(ARRAY_SIZE * sizeof(double));\n",
        "\n",
        "  //timer variables\n",
        "  clock_t start, end;\n",
        "\n",
        "  //initialize array\n",
        "  for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "    X[i] = float(1);\n",
        "    Y[i] = float(1);\n",
        "  }\n",
        "\n",
        "  //flush out cache\n",
        "  dotProduct(ARRAY_SIZE, X, Y, dotp);\n",
        "\n",
        "  //time here\n",
        "  start = clock();\n",
        "    dotProduct(ARRAY_SIZE, X, Y, dotp);\n",
        "  end = clock();\n",
        "  double time_taken = ((double)(end-start))*1e6 / CLOCKS_PER_SEC;\n",
        "  //printf(\"C++ function will get %f microseconds for array size %d\\n\", time_taken, ARRAY_SIZE);\n",
        "\n",
        "  //error checking\n",
        "  unsigned int err_count = 0;\n",
        "  double expected_dotp = 0.0;\n",
        "  double actual_dotp = 0.0;\n",
        "\n",
        "  for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "    expected_dotp = expected_dotp + (X[i] * Y[i]);\n",
        "    actual_dotp += dotp[i];\n",
        "    if(dotp[i] != expected_dotp)\n",
        "      err_count++;\n",
        "  }\n",
        "\n",
        "  printf(\"Actual Dot Product: %.2f\\n\", dotp[ARRAY_SIZE-1]);\n",
        "  printf(\"Expected Dot Product: %.2f\\n\", expected_dotp);\n",
        "\n",
        "  printf(\"Ërror count (C++ program): %d\\n\", err_count);\n",
        "\n",
        "  free(X);\n",
        "  free(Y);\n",
        "\n",
        "  return time_taken;\n",
        "\n",
        "}\n",
        "\n",
        "int main(){\n",
        "  int numberOfRuns = 30;\n",
        "  double timeResults[numberOfRuns];\n",
        "  double totalResult = 0;\n",
        "  //long unsigned int array_size = 0;\n",
        "  unsigned int array_size = 1<<20;\n",
        "\n",
        "\n",
        "  for (int i = 0; i < numberOfRuns; i++) {\n",
        "    timeResults[i] = bulk_logic(array_size);\n",
        "    totalResult += timeResults[i];\n",
        "  }\n",
        "\n",
        "  printf(\"\\n\\nRun results (%d runs):\\n\", numberOfRuns);\n",
        "  for (int i = 0; i < numberOfRuns; i++)\n",
        "    printf(\"Run %d - %f\\n\", i+1, timeResults[i]);\n",
        "\n",
        "  double aveResult = totalResult / numberOfRuns;\n",
        "  printf(\"\\nThe average run time result (%d runs) for array size of %d is %f microseconds\",\n",
        "        numberOfRuns, array_size, aveResult);\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRQXZEvodqid",
        "outputId": "62b1bcc9-ea58-4aff-c3a5-e7bcd83d6323"
      },
      "outputs": [
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "g++ c_dotproduct.cpp -o c_dotproduct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh_bmO_4ebId",
        "outputId": "10ba5b18-9706-4c40-f4c9-d5608019a878"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "Ërror count (C++ program): 0\n",
            "\n",
            "\n",
            "Run results (30 runs):\n",
            "Run 1 - 3142.000000\n",
            "Run 2 - 3164.000000\n",
            "Run 3 - 4065.000000\n",
            "Run 4 - 3258.000000\n",
            "Run 5 - 4818.000000\n",
            "Run 6 - 3167.000000\n",
            "Run 7 - 3170.000000\n",
            "Run 8 - 3403.000000\n",
            "Run 9 - 3682.000000\n",
            "Run 10 - 3438.000000\n",
            "Run 11 - 3216.000000\n",
            "Run 12 - 3155.000000\n",
            "Run 13 - 3162.000000\n",
            "Run 14 - 3153.000000\n",
            "Run 15 - 5647.000000\n",
            "Run 16 - 3232.000000\n",
            "Run 17 - 3255.000000\n",
            "Run 18 - 3253.000000\n",
            "Run 19 - 3168.000000\n",
            "Run 20 - 3208.000000\n",
            "Run 21 - 3322.000000\n",
            "Run 22 - 3165.000000\n",
            "Run 23 - 3222.000000\n",
            "Run 24 - 3214.000000\n",
            "Run 25 - 3224.000000\n",
            "Run 26 - 3203.000000\n",
            "Run 27 - 3875.000000\n",
            "Run 28 - 4444.000000\n",
            "Run 29 - 3163.000000\n",
            "Run 30 - 3161.000000\n",
            "\n",
            "The average run time result (30 runs) for array size of 1048576 is 3458.300000 microseconds"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "./c_dotproduct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOz9GkGENi_s"
      },
      "source": [
        "####vector size: 2^22"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDOV7cGzNi_t",
        "outputId": "c9f6d3e1-609e-4fdc-e037-5ebed89fd343"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting c_dotproduct.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile c_dotproduct.cpp\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "\n",
        "//C++ Function\n",
        "void dotProduct(int n, float* X, float* Y, double* dotp){\n",
        "  dotp[0] = 0.0;\n",
        "  for(int i = 0; i < n; i++){\n",
        "    dotp[i] = dotp[i-1] + (X[i] * Y[i]);\n",
        "  }\n",
        "}\n",
        "\n",
        "//logic\n",
        "double bulk_logic(unsigned int array_size){\n",
        "  const unsigned int ARRAY_SIZE = array_size;\n",
        "  const unsigned int ARRAY_BYTES = ARRAY_SIZE * sizeof(float);\n",
        "\n",
        "  //declare array\n",
        "  float *X, *Y;\n",
        "  double *dotp;\n",
        "  X = (float*)malloc(ARRAY_BYTES);\n",
        "  Y = (float*)malloc(ARRAY_BYTES);\n",
        "  dotp = (double*)malloc(ARRAY_SIZE * sizeof(double));\n",
        "\n",
        "  //timer variables\n",
        "  clock_t start, end;\n",
        "\n",
        "  //initialize array\n",
        "  for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "    X[i] = float(1);\n",
        "    Y[i] = float(1);\n",
        "  }\n",
        "\n",
        "  //flush out cache\n",
        "  dotProduct(ARRAY_SIZE, X, Y, dotp);\n",
        "\n",
        "  //time here\n",
        "  start = clock();\n",
        "    dotProduct(ARRAY_SIZE, X, Y, dotp);\n",
        "  end = clock();\n",
        "  double time_taken = ((double)(end-start))*1e6 / CLOCKS_PER_SEC;\n",
        "  //printf(\"C++ function will get %f microseconds for array size %d\\n\", time_taken, ARRAY_SIZE);\n",
        "\n",
        "  //error checking\n",
        "  unsigned int err_count = 0;\n",
        "  double expected_dotp = 0.0;\n",
        "  double actual_dotp = 0.0;\n",
        "\n",
        "  for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "    expected_dotp = expected_dotp + (X[i] * Y[i]);\n",
        "    actual_dotp += dotp[i];\n",
        "    if(dotp[i] != expected_dotp)\n",
        "      err_count++;\n",
        "  }\n",
        "\n",
        "  printf(\"Actual Dot Product: %.2f\\n\", dotp[ARRAY_SIZE-1]);\n",
        "  printf(\"Expected Dot Product: %.2f\\n\", expected_dotp);\n",
        "\n",
        "  printf(\"Ërror count (C++ program): %d\\n\", err_count);\n",
        "\n",
        "  free(X);\n",
        "  free(Y);\n",
        "\n",
        "  return time_taken;\n",
        "}\n",
        "\n",
        "int main(){\n",
        "  int numberOfRuns = 30;\n",
        "  double timeResults[numberOfRuns];\n",
        "  double totalResult = 0;\n",
        "  //long unsigned int array_size = 0;\n",
        "  unsigned int array_size = 1<<22;\n",
        "\n",
        "\n",
        "  for (int i = 0; i < numberOfRuns; i++) {\n",
        "    timeResults[i] = bulk_logic(array_size);\n",
        "    totalResult += timeResults[i];\n",
        "  }\n",
        "\n",
        "  printf(\"\\n\\nRun results (%d runs):\\n\", numberOfRuns);\n",
        "  for (int i = 0; i < numberOfRuns; i++)\n",
        "    printf(\"Run %d - %f\\n\", i+1, timeResults[i]);\n",
        "\n",
        "  double aveResult = totalResult / numberOfRuns;\n",
        "  printf(\"\\nThe average run time result (%d runs) for array size of %d is %f microseconds\",\n",
        "        numberOfRuns, array_size, aveResult);\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAhedv_WNi_t",
        "outputId": "258a40cf-8220-45f5-9394-45b54515577a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "g++ c_dotproduct.cpp -o c_dotproduct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbFjJNATNi_t",
        "outputId": "d24aba3d-69ce-4cba-d7b7-c01cfef5bd68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "Ërror count (C++ program): 0\n",
            "\n",
            "\n",
            "Run results (30 runs):\n",
            "Run 1 - 12870.000000\n",
            "Run 2 - 14626.000000\n",
            "Run 3 - 12682.000000\n",
            "Run 4 - 13536.000000\n",
            "Run 5 - 13183.000000\n",
            "Run 6 - 14849.000000\n",
            "Run 7 - 14108.000000\n",
            "Run 8 - 13683.000000\n",
            "Run 9 - 14418.000000\n",
            "Run 10 - 13044.000000\n",
            "Run 11 - 15412.000000\n",
            "Run 12 - 13681.000000\n",
            "Run 13 - 13577.000000\n",
            "Run 14 - 13308.000000\n",
            "Run 15 - 14673.000000\n",
            "Run 16 - 13767.000000\n",
            "Run 17 - 13005.000000\n",
            "Run 18 - 12797.000000\n",
            "Run 19 - 12856.000000\n",
            "Run 20 - 18846.000000\n",
            "Run 21 - 12800.000000\n",
            "Run 22 - 12934.000000\n",
            "Run 23 - 12989.000000\n",
            "Run 24 - 14449.000000\n",
            "Run 25 - 13153.000000\n",
            "Run 26 - 12887.000000\n",
            "Run 27 - 13164.000000\n",
            "Run 28 - 12761.000000\n",
            "Run 29 - 12822.000000\n",
            "Run 30 - 15630.000000\n",
            "\n",
            "The average run time result (30 runs) for array size of 4194304 is 13750.333333 microseconds"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "./c_dotproduct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTsMqFCGNsxC"
      },
      "source": [
        "####vector size: 2^24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWS_KpKLNhtt",
        "outputId": "d2d3c71f-dcb0-41bf-fd87-17a29fce3bb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting c_dotproduct.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile c_dotproduct.cpp\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "\n",
        "//C++ Function\n",
        "void dotProduct(int n, float* X, float* Y, double* dotp){\n",
        "  dotp[0] = 0.0;\n",
        "  for(int i = 0; i < n; i++){\n",
        "    dotp[i] = dotp[i-1] + (X[i] * Y[i]);\n",
        "  }\n",
        "}\n",
        "\n",
        "//logic\n",
        "double bulk_logic(unsigned int array_size){\n",
        "  const unsigned int ARRAY_SIZE = array_size;\n",
        "  const unsigned int ARRAY_BYTES = ARRAY_SIZE * sizeof(float);\n",
        "\n",
        "  //declare array\n",
        "  float *X, *Y;\n",
        "  double *dotp;\n",
        "  X = (float*)malloc(ARRAY_BYTES);\n",
        "  Y = (float*)malloc(ARRAY_BYTES);\n",
        "  dotp = (double*)malloc(ARRAY_SIZE * sizeof(double));\n",
        "\n",
        "  //timer variables\n",
        "  clock_t start, end;\n",
        "\n",
        "  //initialize array\n",
        "  for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "    X[i] = float(1);\n",
        "    Y[i] = float(1);\n",
        "  }\n",
        "\n",
        "  //flush out cache\n",
        "  dotProduct(ARRAY_SIZE, X, Y, dotp);\n",
        "\n",
        "  //time here\n",
        "  start = clock();\n",
        "    dotProduct(ARRAY_SIZE, X, Y, dotp);\n",
        "  end = clock();\n",
        "  double time_taken = ((double)(end-start))*1e6 / CLOCKS_PER_SEC;\n",
        "  //printf(\"C++ function will get %f microseconds for array size %d\\n\", time_taken, ARRAY_SIZE);\n",
        "\n",
        "  //error checking\n",
        "  unsigned int err_count = 0;\n",
        "  double expected_dotp = 0.0;\n",
        "  double actual_dotp = 0.0;\n",
        "\n",
        "  for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "    expected_dotp = expected_dotp + (X[i] * Y[i]);\n",
        "    actual_dotp += dotp[i];\n",
        "    if(dotp[i] != expected_dotp)\n",
        "      err_count++;\n",
        "  }\n",
        "\n",
        "  printf(\"Actual Dot Product: %.2f\\n\", dotp[ARRAY_SIZE-1]);\n",
        "  printf(\"Expected Dot Product: %.2f\\n\", expected_dotp);\n",
        "\n",
        "  printf(\"Ërror count (C++ program): %d\\n\", err_count);\n",
        "\n",
        "  free(X);\n",
        "  free(Y);\n",
        "\n",
        "  return time_taken;\n",
        "}\n",
        "\n",
        "int main(){\n",
        "  int numberOfRuns = 30;\n",
        "  double timeResults[numberOfRuns];\n",
        "  double totalResult = 0;\n",
        "  //long unsigned int array_size = 0;\n",
        "  unsigned int array_size = 1<<24;\n",
        "\n",
        "\n",
        "  for (int i = 0; i < numberOfRuns; i++) {\n",
        "    timeResults[i] = bulk_logic(array_size);\n",
        "    totalResult += timeResults[i];\n",
        "  }\n",
        "\n",
        "  printf(\"\\n\\nRun results (%d runs):\\n\", numberOfRuns);\n",
        "  for (int i = 0; i < numberOfRuns; i++)\n",
        "    printf(\"Run %d - %f\\n\", i+1, timeResults[i]);\n",
        "\n",
        "  double aveResult = totalResult / numberOfRuns;\n",
        "  printf(\"\\nThe average run time result (%d runs) for array size of %d is %f microseconds\",\n",
        "        numberOfRuns, array_size, aveResult);\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVpN8jJWNhtu",
        "outputId": "f8ee3082-6098-472a-a3ed-759d1da369b3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "g++ c_dotproduct.cpp -o c_dotproduct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IamMmXlNhtv",
        "outputId": "c3a04560-6ded-4b71-8f87-744364d919ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "Ërror count (C++ program): 0\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "Ërror count (C++ program): 0\n",
            "\n",
            "\n",
            "Run results (30 runs):\n",
            "Run 1 - 52987.000000\n",
            "Run 2 - 52308.000000\n",
            "Run 3 - 54237.000000\n",
            "Run 4 - 58635.000000\n",
            "Run 5 - 54677.000000\n",
            "Run 6 - 57569.000000\n",
            "Run 7 - 58321.000000\n",
            "Run 8 - 52703.000000\n",
            "Run 9 - 55537.000000\n",
            "Run 10 - 55029.000000\n",
            "Run 11 - 51823.000000\n",
            "Run 12 - 60017.000000\n",
            "Run 13 - 53940.000000\n",
            "Run 14 - 55700.000000\n",
            "Run 15 - 55469.000000\n",
            "Run 16 - 52471.000000\n",
            "Run 17 - 61441.000000\n",
            "Run 18 - 54627.000000\n",
            "Run 19 - 54983.000000\n",
            "Run 20 - 69421.000000\n",
            "Run 21 - 60811.000000\n",
            "Run 22 - 56640.000000\n",
            "Run 23 - 51486.000000\n",
            "Run 24 - 53181.000000\n",
            "Run 25 - 56653.000000\n",
            "Run 26 - 57252.000000\n",
            "Run 27 - 94952.000000\n",
            "Run 28 - 95467.000000\n",
            "Run 29 - 95033.000000\n",
            "Run 30 - 96719.000000\n",
            "\n",
            "The average run time result (30 runs) for array size of 16777216 is 61336.300000 microseconds"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "./c_dotproduct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZEZgtrGR6me"
      },
      "source": [
        "####1 run only\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isxpGKjrR6mf",
        "outputId": "5f8e8bc9-e375-44e9-e3d3-191e068bd7bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting c_dotproduct.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile c_dotproduct.cpp\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "\n",
        "//C++ Function\n",
        "void dotProduct(int n, float* X, float* Y, double* dotp){\n",
        "  dotp[0] = 0.0;\n",
        "  for(int i = 0; i < n; i++){\n",
        "    dotp[i] = dotp[i-1] + (X[i] * Y[i]);\n",
        "  }\n",
        "}\n",
        "\n",
        "int main(){\n",
        "  const unsigned int ARRAY_SIZE = 1<<22;\n",
        "  const unsigned int ARRAY_BYTES = ARRAY_SIZE * sizeof(float);\n",
        "\n",
        "  //declare array\n",
        "  float *X, *Y;\n",
        "  double *dotp;\n",
        "  X = (float*)malloc(ARRAY_BYTES);\n",
        "  Y = (float*)malloc(ARRAY_BYTES);\n",
        "  dotp = (double*)malloc(ARRAY_SIZE * sizeof(double));\n",
        "\n",
        "  //timer variables\n",
        "  clock_t start, end;\n",
        "\n",
        "  //initialize array\n",
        "  for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "    X[i] = float(1);\n",
        "    Y[i] = float(1);\n",
        "  }\n",
        "\n",
        "  //flush out cache\n",
        "  dotProduct(ARRAY_SIZE, X, Y, dotp);\n",
        "\n",
        "  //time here\n",
        "  start = clock();\n",
        "    dotProduct(ARRAY_SIZE, X, Y, dotp);\n",
        "  end = clock();\n",
        "  double time_taken = ((double)(end-start))*1e6 / CLOCKS_PER_SEC;\n",
        "  printf(\"C++ function will get %f microseconds for array size %d\\n\", time_taken, ARRAY_SIZE);\n",
        "\n",
        "  //error checking\n",
        "  unsigned int err_count = 0;\n",
        "  double expected_dotp = 0.0;\n",
        "  double actual_dotp = 0.0;\n",
        "\n",
        "  for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "    expected_dotp = expected_dotp + (X[i] * Y[i]);\n",
        "    actual_dotp += dotp[i];\n",
        "    if(dotp[i] != expected_dotp)\n",
        "      err_count++;\n",
        "  }\n",
        "\n",
        "  printf(\"Actual Dot Product: %.2f\\n\", dotp[ARRAY_SIZE-1]);\n",
        "  printf(\"Expected Dot Product: %.2f\\n\", expected_dotp);\n",
        "\n",
        "  printf(\"Ërror count (C++ program): %d\\n\", err_count);\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ihn1MrWvR6mh",
        "outputId": "98b337b0-0782-4c8d-95c9-31aea8f219b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "g++ c_dotproduct.cpp -o c_dotproduct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9IcWAAFR6mh",
        "outputId": "19bba069-194a-4772-d252-3fb56acfbcb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C++ function will get 15242.000000 microseconds for array size 4194304\n",
            "Actual Dot Product: 24595667561819402240.00\n",
            "Expected Dot Product: 24595667561819402240.00\n",
            "Ërror count (C++ program): 0\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "./c_dotproduct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzyliJM3jxzK"
      },
      "source": [
        "##CUDA program WITHOUT cooperative group using grid-stride loop with prefetching+mem advise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4FVFIZMS4vl"
      },
      "source": [
        "### 256 threads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TuQBAg1TCj1"
      },
      "source": [
        "#### vector size: 2^20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMRgpIOrUaug",
        "outputId": "53a47b01-dff5-40fa-8896-b5cacaf8f4cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting cuda_dotproduct1.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile cuda_dotproduct1.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "\n",
        "__device__ double atomicAddDouble(double* address, double val){\n",
        "    unsigned long long int* address_as_ull = (unsigned long long int*)address;\n",
        "    unsigned long long int old = *address_as_ull, assumed;\n",
        "    do {\n",
        "        assumed = old;\n",
        "        old = atomicCAS(address_as_ull, assumed, __double_as_longlong(val + __longlong_as_double(assumed)));\n",
        "    } while (assumed != old);\n",
        "    return __longlong_as_double(old);\n",
        "}\n",
        "\n",
        "__global__\n",
        "void sumDot(int n, int numThreads, float* X, float* Y, double* dotp, double* dotp_temp){\n",
        "\n",
        "  __shared__ double sharedTemp;\n",
        "  sharedTemp = 0.0;\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "\n",
        "  for(int i = index; i < n; i += stride){\n",
        "    __syncthreads();\n",
        "    atomicAddDouble(&sharedTemp, dotp_temp[i]);\n",
        "    //printf(\"index = %d| sharedTemp = %f\\n\", index, sharedTemp);\n",
        "    __syncthreads();\n",
        "    if (index % blockDim.x == 0)\n",
        "    {\n",
        "      atomicAddDouble(dotp, sharedTemp);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__\n",
        "void dotProduct(int n, int numThreads, float* X, float* Y, double* dotp, double* dotp_temp){\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "\n",
        "  // Fill dotp_temp with the product\n",
        "  for(int i = index; i < n; i += stride){\n",
        "    dotp_temp[i] = X[i] * Y[i];\n",
        "  }\n",
        "}\n",
        "\n",
        "int bulk_logic(unsigned int array_size){\n",
        "  const unsigned int ARRAY_SIZE = array_size;\n",
        "  const unsigned int ARRAY_BYTES = ARRAY_SIZE * sizeof(float);\n",
        "\n",
        "  //declare array\n",
        "  float *X, *Y;\n",
        "  double *dotp, *dotp_temp;\n",
        "  cudaMallocManaged(&X, ARRAY_BYTES);\n",
        "  cudaMallocManaged(&Y, ARRAY_BYTES);\n",
        "  cudaMallocManaged(&dotp, sizeof(double));\n",
        "  cudaMallocManaged(&dotp_temp, ARRAY_SIZE * sizeof(double));\n",
        "\n",
        "  //prefetch data to create CPU page memory\n",
        "  int device = -1;\n",
        "  cudaGetDevice(&device);\n",
        "\n",
        "  //memory advise\n",
        "  cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId);\n",
        "\n",
        "  //\"prefetch data\" to create CPU page memory\n",
        "  cudaMemPrefetchAsync(X, ARRAY_BYTES, cudaCpuDeviceId, NULL);\n",
        "  cudaMemPrefetchAsync(Y, ARRAY_BYTES, cudaCpuDeviceId, NULL);\n",
        "\n",
        "  //\"prefetch data\" to create GPU page memory\n",
        "  cudaMemPrefetchAsync(dotp, sizeof(double), device, NULL);\n",
        "  cudaMemPrefetchAsync(dotp_temp, ARRAY_SIZE * sizeof(double), device, NULL);\n",
        "\n",
        "  //initialize array\n",
        "  for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "    X[i] = float(1);\n",
        "    Y[i] = float(1);\n",
        "  }\n",
        "\n",
        "  //(to transfer data from the) Actual transfer: CPU to GPU\n",
        "  cudaMemPrefetchAsync(X, ARRAY_BYTES, device, NULL);\n",
        "  cudaMemPrefetchAsync(Y, ARRAY_BYTES, device, NULL);\n",
        "\n",
        "  //call cuda kernel\n",
        "  int numThreads = 256;\n",
        "  int numBlocks = (ARRAY_SIZE+numThreads-1)/numThreads;\n",
        "  printf(\"numBlocks = %d, numThreads = %d\\n\", numBlocks, numThreads);\n",
        "\n",
        "  dotProduct<<<numBlocks, numThreads>>> (ARRAY_SIZE, numThreads, X, Y, dotp, dotp_temp);\n",
        "  sumDot<<<numBlocks, numThreads>>> (ARRAY_SIZE, numThreads, X, Y, dotp, dotp_temp);\n",
        "\n",
        "  //barrier\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  //prefetch\n",
        "  cudaMemPrefetchAsync(dotp, sizeof(double), cudaCpuDeviceId, NULL);\n",
        "\n",
        "  //error checking\n",
        "  double expected_dotp  =  0.0;\n",
        "  for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "    expected_dotp = expected_dotp + (X[i] * Y[i]);\n",
        "  }\n",
        "\n",
        "  printf(\"Actual Dot Product: %.2f\\n\", *dotp);\n",
        "  printf(\"Expected Dot Product: %.2f\\n\", expected_dotp);\n",
        "\n",
        "  if(*dotp != expected_dotp){\n",
        "    printf(\"ERROR!: Actual Dot Product and Expected Dot Product is NOT EQUAL\\n\");\n",
        "  }\n",
        "  else{\n",
        "    printf(\"NO ERROR!\\n\");\n",
        "  }\n",
        "\n",
        "  //free memory\n",
        "  cudaFree(X);\n",
        "  cudaFree(Y);\n",
        "  cudaFree(dotp);\n",
        "  cudaFree(dotp_temp);\n",
        "\n",
        "  return 0;\n",
        "}\n",
        "\n",
        "\n",
        "int main(){\n",
        "  int numberOfRuns = 30;\n",
        "  //############CHANGE HERE######################\n",
        "  unsigned int array_size = 1<<20;\n",
        "  //############CHANGE HERE######################\n",
        "\n",
        "  for (int i = 0; i < numberOfRuns; i++) {\n",
        "    bulk_logic(array_size);\n",
        "  }\n",
        "return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGgczNsyUauh",
        "outputId": "b51c33be-20bf-4a72-c401-fbf37cb6526f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "nvcc cuda_dotproduct1.cu -o cuda_dotproduct1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0vMhEIlUauh",
        "outputId": "17902eb5-ea72-4b42-b664-b83f8afee2f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==10744== NVPROF is profiling process 10744, command: ./cuda_dotproduct1\n",
            "numBlocks = 4096, numThreads = 256\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 256\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 256\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 256\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 256\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 256\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 256\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 256\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 256\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 256\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 256\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 256\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 256\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 256\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 256\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 256\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 256\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 256\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 256\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 256\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 256\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 256\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 256\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 256\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 256\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 256\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 256\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 256\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 256\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 256\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "==10744== Profiling application: ./cuda_dotproduct1\n",
            "==10744== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.43%  337.00ms        30  11.233ms  6.8292ms  17.790ms  sumDot(int, int, float*, float*, double*, double*)\n",
            "                    0.57%  1.9323ms        30  64.410us  61.375us  69.407us  dotProduct(int, int, float*, float*, double*, double*)\n",
            "      API calls:   52.77%  367.88ms        30  12.263ms  7.8763ms  18.856ms  cudaDeviceSynchronize\n",
            "                   35.01%  244.05ms       120  2.0338ms  8.0910us  242.16ms  cudaMallocManaged\n",
            "                    8.25%  57.526ms       210  273.93us  11.704us  1.0926ms  cudaMemPrefetchAsync\n",
            "                    3.74%  26.041ms       120  217.01us  43.371us  488.08us  cudaFree\n",
            "                    0.17%  1.2124ms        60  20.206us  6.4840us  110.88us  cudaLaunchKernel\n",
            "                    0.04%  265.00us       120  2.2080us     997ns  13.520us  cudaMemAdvise\n",
            "                    0.02%  119.61us       101  1.1840us     125ns  49.406us  cuDeviceGetAttribute\n",
            "                    0.01%  45.618us        30  1.5200us     927ns  2.2960us  cudaGetDevice\n",
            "                    0.00%  24.672us         1  24.672us  24.672us  24.672us  cuDeviceGetName\n",
            "                    0.00%  7.8020us         1  7.8020us  7.8020us  7.8020us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.0240us         3     674ns     288ns  1.4010us  cuDeviceGetCount\n",
            "                    0.00%  1.0720us         2     536ns     269ns     803ns  cuDeviceGet\n",
            "                    0.00%     450ns         1     450ns     450ns     450ns  cuModuleGetLoadingMode\n",
            "                    0.00%     342ns         1     342ns     342ns     342ns  cuDeviceTotalMem\n",
            "                    0.00%     217ns         1     217ns     217ns     217ns  cuDeviceGetUuid\n",
            "\n",
            "==10744== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "     120  2.0000MB  2.0000MB  2.0000MB  240.0000MB  21.32465ms  Host To Device\n",
            "      30  4.0000KB  4.0000KB  4.0000KB  120.0000KB  55.16800us  Device To Host\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "nvprof ./cuda_dotproduct1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tczpcpcYRFhp"
      },
      "source": [
        "#### vector size: 2^22"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FprKqTikRFh4",
        "outputId": "a3164755-af99-4d53-af9e-4e0e5ca4b9cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting cuda_dotproduct1.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile cuda_dotproduct1.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "\n",
        "__device__ double atomicAddDouble(double* address, double val){\n",
        "    unsigned long long int* address_as_ull = (unsigned long long int*)address;\n",
        "    unsigned long long int old = *address_as_ull, assumed;\n",
        "    do {\n",
        "        assumed = old;\n",
        "        old = atomicCAS(address_as_ull, assumed, __double_as_longlong(val + __longlong_as_double(assumed)));\n",
        "    } while (assumed != old);\n",
        "    return __longlong_as_double(old);\n",
        "}\n",
        "\n",
        "__global__\n",
        "void sumDot(int n, int numThreads, float* X, float* Y, double* dotp, double* dotp_temp){\n",
        "\n",
        "  __shared__ double sharedTemp;\n",
        "  sharedTemp = 0.0;\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "\n",
        "  for(int i = index; i < n; i += stride){\n",
        "    __syncthreads();\n",
        "    atomicAddDouble(&sharedTemp, dotp_temp[i]);\n",
        "    //printf(\"index = %d| sharedTemp = %f\\n\", index, sharedTemp);\n",
        "    __syncthreads();\n",
        "    if (index % blockDim.x == 0)\n",
        "    {\n",
        "      atomicAddDouble(dotp, sharedTemp);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__\n",
        "void dotProduct(int n, int numThreads, float* X, float* Y, double* dotp, double* dotp_temp){\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "\n",
        "  // Fill dotp_temp with the product\n",
        "  for(int i = index; i < n; i += stride){\n",
        "    dotp_temp[i] = X[i] * Y[i];\n",
        "  }\n",
        "}\n",
        "\n",
        "int bulk_logic(unsigned int array_size){\n",
        "  const unsigned int ARRAY_SIZE = array_size;\n",
        "  const unsigned int ARRAY_BYTES = ARRAY_SIZE * sizeof(float);\n",
        "\n",
        "  //declare array\n",
        "  float *X, *Y;\n",
        "  double *dotp, *dotp_temp;\n",
        "  cudaMallocManaged(&X, ARRAY_BYTES);\n",
        "  cudaMallocManaged(&Y, ARRAY_BYTES);\n",
        "  cudaMallocManaged(&dotp, sizeof(double));\n",
        "  cudaMallocManaged(&dotp_temp, ARRAY_SIZE * sizeof(double));\n",
        "\n",
        "  //prefetch data to create CPU page memory\n",
        "  int device = -1;\n",
        "  cudaGetDevice(&device);\n",
        "\n",
        "  //memory advise\n",
        "  cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId);\n",
        "\n",
        "  //\"prefetch data\" to create CPU page memory\n",
        "  cudaMemPrefetchAsync(X, ARRAY_BYTES, cudaCpuDeviceId, NULL);\n",
        "  cudaMemPrefetchAsync(Y, ARRAY_BYTES, cudaCpuDeviceId, NULL);\n",
        "\n",
        "  //\"prefetch data\" to create GPU page memory\n",
        "  cudaMemPrefetchAsync(dotp, sizeof(double), device, NULL);\n",
        "  cudaMemPrefetchAsync(dotp_temp, ARRAY_SIZE * sizeof(double), device, NULL);\n",
        "\n",
        "  //initialize array\n",
        "  for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "    X[i] = float(1);\n",
        "    Y[i] = float(1);\n",
        "  }\n",
        "\n",
        "  //(to transfer data from the) Actual transfer: CPU to GPU\n",
        "  cudaMemPrefetchAsync(X, ARRAY_BYTES, device, NULL);\n",
        "  cudaMemPrefetchAsync(Y, ARRAY_BYTES, device, NULL);\n",
        "\n",
        "  //call cuda kernel\n",
        "  int numThreads = 256;\n",
        "  int numBlocks = (ARRAY_SIZE+numThreads-1)/numThreads;\n",
        "  printf(\"numBlocks = %d, numThreads = %d\\n\", numBlocks, numThreads);\n",
        "\n",
        "  dotProduct<<<numBlocks, numThreads>>> (ARRAY_SIZE, numThreads, X, Y, dotp, dotp_temp);\n",
        "  sumDot<<<numBlocks, numThreads>>> (ARRAY_SIZE, numThreads, X, Y, dotp, dotp_temp);\n",
        "\n",
        "  //barrier\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  //prefetch\n",
        "  cudaMemPrefetchAsync(dotp, sizeof(double), cudaCpuDeviceId, NULL);\n",
        "\n",
        "  //error checking\n",
        "  double expected_dotp  =  0.0;\n",
        "  for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "    expected_dotp = expected_dotp + (X[i] * Y[i]);\n",
        "  }\n",
        "\n",
        "  printf(\"Actual Dot Product: %.2f\\n\", *dotp);\n",
        "  printf(\"Expected Dot Product: %.2f\\n\", expected_dotp);\n",
        "\n",
        "  if(*dotp != expected_dotp){\n",
        "    printf(\"ERROR!: Actual Dot Product and Expected Dot Product is NOT EQUAL\\n\");\n",
        "  }\n",
        "  else{\n",
        "    printf(\"NO ERROR!\\n\");\n",
        "  }\n",
        "\n",
        "  //free memory\n",
        "  cudaFree(X);\n",
        "  cudaFree(Y);\n",
        "  cudaFree(dotp);\n",
        "  cudaFree(dotp_temp);\n",
        "\n",
        "  return 0;\n",
        "}\n",
        "\n",
        "\n",
        "int main(){\n",
        "  int numberOfRuns = 30;\n",
        "  //############CHANGE HERE######################\n",
        "  unsigned int array_size = 1<<22;\n",
        "  //############CHANGE HERE######################\n",
        "\n",
        "  for (int i = 0; i < numberOfRuns; i++) {\n",
        "    bulk_logic(array_size);\n",
        "  }\n",
        "return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rw6LbKnNRFh4",
        "outputId": "310d3ead-282d-4c9a-e2bd-12d3e40067de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "nvcc cuda_dotproduct1.cu -o cuda_dotproduct1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1W5ZmWaRFh4",
        "outputId": "e56bee34-fa4e-4abc-ed2c-f052c31726e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==11157== NVPROF is profiling process 11157, command: ./cuda_dotproduct1\n",
            "numBlocks = 16384, numThreads = 256\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 256\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 256\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 256\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 256\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 256\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 256\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 256\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 256\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 256\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 256\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 256\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 256\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 256\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 256\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 256\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 256\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 256\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 256\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 256\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 256\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 256\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 256\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 256\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 256\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 256\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 256\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 256\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 256\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 256\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "==11157== Profiling application: ./cuda_dotproduct1\n",
            "==11157== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.19%  982.38ms        30  32.746ms  26.130ms  71.323ms  sumDot(int, int, float*, float*, double*, double*)\n",
            "                    0.81%  8.0214ms        30  267.38us  263.13us  269.69us  dotProduct(int, int, float*, float*, double*, double*)\n",
            "      API calls:   57.30%  1.02549s        30  34.183ms  26.441ms  71.591ms  cudaDeviceSynchronize\n",
            "                   21.11%  377.84ms       210  1.7993ms  12.615us  6.8675ms  cudaMemPrefetchAsync\n",
            "                   16.03%  286.88ms       120  2.3907ms  8.6640us  284.20ms  cudaMallocManaged\n",
            "                    5.43%  97.112ms       120  809.26us  58.443us  2.1163ms  cudaFree\n",
            "                    0.10%  1.8242ms        60  30.404us  8.2160us  81.245us  cudaLaunchKernel\n",
            "                    0.02%  334.38us       120  2.7860us  1.0310us  16.968us  cudaMemAdvise\n",
            "                    0.01%  185.55us       101  1.8370us     201ns  68.921us  cuDeviceGetAttribute\n",
            "                    0.00%  73.960us        30  2.4650us  1.8890us  3.0370us  cudaGetDevice\n",
            "                    0.00%  29.002us         1  29.002us  29.002us  29.002us  cuDeviceGetName\n",
            "                    0.00%  8.4980us         1  8.4980us  8.4980us  8.4980us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.7640us         3     921ns     313ns  2.0760us  cuDeviceGetCount\n",
            "                    0.00%     997ns         2     498ns     288ns     709ns  cuDeviceGet\n",
            "                    0.00%     671ns         1     671ns     671ns     671ns  cuDeviceTotalMem\n",
            "                    0.00%     478ns         1     478ns     478ns     478ns  cuModuleGetLoadingMode\n",
            "                    0.00%     400ns         1     400ns     400ns     400ns  cuDeviceGetUuid\n",
            "\n",
            "==11157== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "     480  2.0000MB  2.0000MB  2.0000MB  960.0000MB  85.64831ms  Host To Device\n",
            "      30  4.0000KB  4.0000KB  4.0000KB  120.0000KB  49.31200us  Device To Host\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "nvprof ./cuda_dotproduct1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BduQmJubPofx"
      },
      "source": [
        "#### vector size: 2^24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wT9b0du5Pof4",
        "outputId": "5e866b9f-ae21-4a17-f7c5-06ef3e36ce09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing cuda_dotproduct1.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile cuda_dotproduct1.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "\n",
        "__device__ double atomicAddDouble(double* address, double val){\n",
        "    unsigned long long int* address_as_ull = (unsigned long long int*)address;\n",
        "    unsigned long long int old = *address_as_ull, assumed;\n",
        "    do {\n",
        "        assumed = old;\n",
        "        old = atomicCAS(address_as_ull, assumed, __double_as_longlong(val + __longlong_as_double(assumed)));\n",
        "    } while (assumed != old);\n",
        "    return __longlong_as_double(old);\n",
        "}\n",
        "\n",
        "__global__\n",
        "void sumDot(int n, int numThreads, float* X, float* Y, double* dotp, double* dotp_temp){\n",
        "\n",
        "  __shared__ double sharedTemp;\n",
        "  sharedTemp = 0.0;\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "\n",
        "  for(int i = index; i < n; i += stride){\n",
        "    __syncthreads();\n",
        "    atomicAddDouble(&sharedTemp, dotp_temp[i]);\n",
        "    //printf(\"index = %d| sharedTemp = %f\\n\", index, sharedTemp);\n",
        "    __syncthreads();\n",
        "    if (index % blockDim.x == 0)\n",
        "    {\n",
        "      atomicAddDouble(dotp, sharedTemp);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__\n",
        "void dotProduct(int n, int numThreads, float* X, float* Y, double* dotp, double* dotp_temp){\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "\n",
        "  // Fill dotp_temp with the product\n",
        "  for(int i = index; i < n; i += stride){\n",
        "    dotp_temp[i] = X[i] * Y[i];\n",
        "  }\n",
        "}\n",
        "\n",
        "int bulk_logic(unsigned int array_size){\n",
        "  const unsigned int ARRAY_SIZE = array_size;\n",
        "  const unsigned int ARRAY_BYTES = ARRAY_SIZE * sizeof(float);\n",
        "\n",
        "  //declare array\n",
        "  float *X, *Y;\n",
        "  double *dotp, *dotp_temp;\n",
        "  cudaMallocManaged(&X, ARRAY_BYTES);\n",
        "  cudaMallocManaged(&Y, ARRAY_BYTES);\n",
        "  cudaMallocManaged(&dotp, sizeof(double));\n",
        "  cudaMallocManaged(&dotp_temp, ARRAY_SIZE * sizeof(double));\n",
        "\n",
        "  //prefetch data to create CPU page memory\n",
        "  int device = -1;\n",
        "  cudaGetDevice(&device);\n",
        "\n",
        "  //memory advise\n",
        "  cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId);\n",
        "\n",
        "  //\"prefetch data\" to create CPU page memory\n",
        "  cudaMemPrefetchAsync(X, ARRAY_BYTES, cudaCpuDeviceId, NULL);\n",
        "  cudaMemPrefetchAsync(Y, ARRAY_BYTES, cudaCpuDeviceId, NULL);\n",
        "\n",
        "  //\"prefetch data\" to create GPU page memory\n",
        "  cudaMemPrefetchAsync(dotp, sizeof(double), device, NULL);\n",
        "  cudaMemPrefetchAsync(dotp_temp, ARRAY_SIZE * sizeof(double), device, NULL);\n",
        "\n",
        "  //initialize array\n",
        "  for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "    X[i] = float(1);\n",
        "    Y[i] = float(1);\n",
        "  }\n",
        "\n",
        "  //(to transfer data from the) Actual transfer: CPU to GPU\n",
        "  cudaMemPrefetchAsync(X, ARRAY_BYTES, device, NULL);\n",
        "  cudaMemPrefetchAsync(Y, ARRAY_BYTES, device, NULL);\n",
        "\n",
        "  //call cuda kernel\n",
        "  int numThreads = 256;\n",
        "  int numBlocks = (ARRAY_SIZE+numThreads-1)/numThreads;\n",
        "  printf(\"numBlocks = %d, numThreads = %d\\n\", numBlocks, numThreads);\n",
        "\n",
        "  dotProduct<<<numBlocks, numThreads>>> (ARRAY_SIZE, numThreads, X, Y, dotp, dotp_temp);\n",
        "  sumDot<<<numBlocks, numThreads>>> (ARRAY_SIZE, numThreads, X, Y, dotp, dotp_temp);\n",
        "\n",
        "  //barrier\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  //prefetch\n",
        "  cudaMemPrefetchAsync(dotp, sizeof(double), cudaCpuDeviceId, NULL);\n",
        "\n",
        "  //error checking\n",
        "  double expected_dotp  =  0.0;\n",
        "  for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "    expected_dotp = expected_dotp + (X[i] * Y[i]);\n",
        "  }\n",
        "\n",
        "  printf(\"Actual Dot Product: %.2f\\n\", *dotp);\n",
        "  printf(\"Expected Dot Product: %.2f\\n\", expected_dotp);\n",
        "\n",
        "  if(*dotp != expected_dotp){\n",
        "    printf(\"ERROR!: Actual Dot Product and Expected Dot Product is NOT EQUAL\\n\");\n",
        "  }\n",
        "  else{\n",
        "    printf(\"NO ERROR!\\n\");\n",
        "  }\n",
        "\n",
        "  //free memory\n",
        "  cudaFree(X);\n",
        "  cudaFree(Y);\n",
        "  cudaFree(dotp);\n",
        "  cudaFree(dotp_temp);\n",
        "\n",
        "  return 0;\n",
        "}\n",
        "\n",
        "\n",
        "int main(){\n",
        "  int numberOfRuns = 30;\n",
        "  //############CHANGE HERE######################\n",
        "  unsigned int array_size = 1<<24;\n",
        "  //############CHANGE HERE######################\n",
        "\n",
        "  for (int i = 0; i < numberOfRuns; i++) {\n",
        "    bulk_logic(array_size);\n",
        "  }\n",
        "return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMtqNpGZPof5",
        "outputId": "89894df0-ad39-4710-9734-f292f4e1560c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "nvcc cuda_dotproduct1.cu -o cuda_dotproduct1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aztc7B-APof5",
        "outputId": "ddd69b9f-edc4-4bd4-d73b-7ffd50b7b450"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==540== NVPROF is profiling process 540, command: ./cuda_dotproduct1\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "==540== Profiling application: ./cuda_dotproduct1\n",
            "==540== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.02%  3.30416s        30  110.14ms  105.03ms  245.04ms  sumDot(int, int, float*, float*, double*, double*)\n",
            "                    0.98%  32.826ms        30  1.0942ms  1.0703ms  1.0989ms  dotProduct(int, int, float*, float*, double*, double*)\n",
            "      API calls:   55.98%  3.33712s        30  111.24ms  106.12ms  246.11ms  cudaDeviceSynchronize\n",
            "                   28.85%  1.71970s       210  8.1891ms  41.748us  48.940ms  cudaMemPrefetchAsync\n",
            "                    8.22%  490.20ms       120  4.0850ms  12.020us  485.63ms  cudaMallocManaged\n",
            "                    6.88%  410.18ms       120  3.4182ms  63.177us  13.741ms  cudaFree\n",
            "                    0.03%  1.8259ms        60  30.431us  8.0890us  73.373us  cudaLaunchKernel\n",
            "                    0.02%  1.0365ms         1  1.0365ms  1.0365ms  1.0365ms  cuDeviceGetPCIBusId\n",
            "                    0.01%  418.72us       120  3.4890us  1.0020us  89.673us  cudaMemAdvise\n",
            "                    0.00%  181.25us       101  1.7940us     258ns  66.470us  cuDeviceGetAttribute\n",
            "                    0.00%  85.305us        30  2.8430us  2.3010us  4.2490us  cudaGetDevice\n",
            "                    0.00%  36.438us         1  36.438us  36.438us  36.438us  cuDeviceGetName\n",
            "                    0.00%  2.6210us         3     873ns     322ns  1.9750us  cuDeviceGetCount\n",
            "                    0.00%  1.0120us         2     506ns     352ns     660ns  cuDeviceGet\n",
            "                    0.00%     901ns         1     901ns     901ns     901ns  cuModuleGetLoadingMode\n",
            "                    0.00%     746ns         1     746ns     746ns     746ns  cuDeviceTotalMem\n",
            "                    0.00%     382ns         1     382ns     382ns     382ns  cuDeviceGetUuid\n",
            "\n",
            "==540== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "    1920  2.0000MB  2.0000MB  2.0000MB  3.750000GB  335.5646ms  Host To Device\n",
            "      30  4.0000KB  4.0000KB  4.0000KB  120.0000KB  44.64000us  Device To Host\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "nvprof ./cuda_dotproduct1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDVT7ioSRwhA"
      },
      "source": [
        "### 512 threads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Lv6gHpQRwhH"
      },
      "source": [
        "#### vector size: 2^20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYdrjiOhRwhI",
        "outputId": "7263f2b7-0ca3-4e51-8f02-c520a97d8b55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting cuda_dotproduct1.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile cuda_dotproduct1.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "\n",
        "__device__ double atomicAddDouble(double* address, double val){\n",
        "    unsigned long long int* address_as_ull = (unsigned long long int*)address;\n",
        "    unsigned long long int old = *address_as_ull, assumed;\n",
        "    do {\n",
        "        assumed = old;\n",
        "        old = atomicCAS(address_as_ull, assumed, __double_as_longlong(val + __longlong_as_double(assumed)));\n",
        "    } while (assumed != old);\n",
        "    return __longlong_as_double(old);\n",
        "}\n",
        "\n",
        "__global__\n",
        "void sumDot(int n, int numThreads, float* X, float* Y, double* dotp, double* dotp_temp){\n",
        "\n",
        "  __shared__ double sharedTemp;\n",
        "  sharedTemp = 0.0;\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "\n",
        "  for(int i = index; i < n; i += stride){\n",
        "    __syncthreads();\n",
        "    atomicAddDouble(&sharedTemp, dotp_temp[i]);\n",
        "    //printf(\"index = %d| sharedTemp = %f\\n\", index, sharedTemp);\n",
        "    __syncthreads();\n",
        "    if (index % blockDim.x == 0)\n",
        "    {\n",
        "      atomicAddDouble(dotp, sharedTemp);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__\n",
        "void dotProduct(int n, int numThreads, float* X, float* Y, double* dotp, double* dotp_temp){\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "\n",
        "  // Fill dotp_temp with the product\n",
        "  for(int i = index; i < n; i += stride){\n",
        "    dotp_temp[i] = X[i] * Y[i];\n",
        "  }\n",
        "}\n",
        "\n",
        "int bulk_logic(unsigned int array_size){\n",
        "  const unsigned int ARRAY_SIZE = array_size;\n",
        "  const unsigned int ARRAY_BYTES = ARRAY_SIZE * sizeof(float);\n",
        "\n",
        "  //declare array\n",
        "  float *X, *Y;\n",
        "  double *dotp, *dotp_temp;\n",
        "  cudaMallocManaged(&X, ARRAY_BYTES);\n",
        "  cudaMallocManaged(&Y, ARRAY_BYTES);\n",
        "  cudaMallocManaged(&dotp, sizeof(double));\n",
        "  cudaMallocManaged(&dotp_temp, ARRAY_SIZE * sizeof(double));\n",
        "\n",
        "  //prefetch data to create CPU page memory\n",
        "  int device = -1;\n",
        "  cudaGetDevice(&device);\n",
        "\n",
        "  //memory advise\n",
        "  cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId);\n",
        "\n",
        "  //\"prefetch data\" to create CPU page memory\n",
        "  cudaMemPrefetchAsync(X, ARRAY_BYTES, cudaCpuDeviceId, NULL);\n",
        "  cudaMemPrefetchAsync(Y, ARRAY_BYTES, cudaCpuDeviceId, NULL);\n",
        "\n",
        "  //\"prefetch data\" to create GPU page memory\n",
        "  cudaMemPrefetchAsync(dotp, sizeof(double), device, NULL);\n",
        "  cudaMemPrefetchAsync(dotp_temp, ARRAY_SIZE * sizeof(double), device, NULL);\n",
        "\n",
        "  //initialize array\n",
        "  for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "    X[i] = float(1);\n",
        "    Y[i] = float(1);\n",
        "  }\n",
        "\n",
        "  //(to transfer data from the) Actual transfer: CPU to GPU\n",
        "  cudaMemPrefetchAsync(X, ARRAY_BYTES, device, NULL);\n",
        "  cudaMemPrefetchAsync(Y, ARRAY_BYTES, device, NULL);\n",
        "\n",
        "  //call cuda kernel\n",
        "  int numThreads = 512;\n",
        "  int numBlocks = (ARRAY_SIZE+numThreads-1)/numThreads;\n",
        "  printf(\"numBlocks = %d, numThreads = %d\\n\", numBlocks, numThreads);\n",
        "\n",
        "  dotProduct<<<numBlocks, numThreads>>> (ARRAY_SIZE, numThreads, X, Y, dotp, dotp_temp);\n",
        "  sumDot<<<numBlocks, numThreads>>> (ARRAY_SIZE, numThreads, X, Y, dotp, dotp_temp);\n",
        "\n",
        "  //barrier\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  //prefetch\n",
        "  cudaMemPrefetchAsync(dotp, sizeof(double), cudaCpuDeviceId, NULL);\n",
        "\n",
        "  //error checking\n",
        "  double expected_dotp  =  0.0;\n",
        "  for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "    expected_dotp = expected_dotp + (X[i] * Y[i]);\n",
        "  }\n",
        "\n",
        "  printf(\"Actual Dot Product: %.2f\\n\", *dotp);\n",
        "  printf(\"Expected Dot Product: %.2f\\n\", expected_dotp);\n",
        "\n",
        "  if(*dotp != expected_dotp){\n",
        "    printf(\"ERROR!: Actual Dot Product and Expected Dot Product is NOT EQUAL\\n\");\n",
        "  }\n",
        "  else{\n",
        "    printf(\"NO ERROR!\\n\");\n",
        "  }\n",
        "\n",
        "  //free memory\n",
        "  cudaFree(X);\n",
        "  cudaFree(Y);\n",
        "  cudaFree(dotp);\n",
        "  cudaFree(dotp_temp);\n",
        "\n",
        "  return 0;\n",
        "}\n",
        "\n",
        "\n",
        "int main(){\n",
        "  int numberOfRuns = 30;\n",
        "  //############CHANGE HERE######################\n",
        "  unsigned int array_size = 1<<20;\n",
        "  //############CHANGE HERE######################\n",
        "\n",
        "  for (int i = 0; i < numberOfRuns; i++) {\n",
        "    bulk_logic(array_size);\n",
        "  }\n",
        "return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NC0claAZRwhI",
        "outputId": "a9345ad9-caff-4441-b420-f1bb2451b245"
      },
      "outputs": [
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "nvcc cuda_dotproduct1.cu -o cuda_dotproduct1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcxNxAjFRwhI",
        "outputId": "a3e17878-171c-4e6c-afeb-a405cd01e458"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==12977== NVPROF is profiling process 12977, command: ./cuda_dotproduct1\n",
            "numBlocks = 2048, numThreads = 512\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 2048, numThreads = 512\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 2048, numThreads = 512\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 2048, numThreads = 512\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 2048, numThreads = 512\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 2048, numThreads = 512\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 2048, numThreads = 512\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 2048, numThreads = 512\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 2048, numThreads = 512\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 2048, numThreads = 512\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 2048, numThreads = 512\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 2048, numThreads = 512\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 2048, numThreads = 512\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 2048, numThreads = 512\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 2048, numThreads = 512\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 2048, numThreads = 512\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 2048, numThreads = 512\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 2048, numThreads = 512\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 2048, numThreads = 512\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 2048, numThreads = 512\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 2048, numThreads = 512\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 2048, numThreads = 512\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 2048, numThreads = 512\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 2048, numThreads = 512\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 2048, numThreads = 512\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 2048, numThreads = 512\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 2048, numThreads = 512\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 2048, numThreads = 512\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 2048, numThreads = 512\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 2048, numThreads = 512\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "==12977== Profiling application: ./cuda_dotproduct1\n",
            "==12977== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.63%  523.71ms        30  17.457ms  13.000ms  35.522ms  sumDot(int, int, float*, float*, double*, double*)\n",
            "                    0.37%  1.9335ms        30  64.450us  61.951us  73.887us  dotProduct(int, int, float*, float*, double*, double*)\n",
            "      API calls:   54.71%  552.53ms        30  18.418ms  13.085ms  45.860ms  cudaDeviceSynchronize\n",
            "                   29.22%  295.08ms       120  2.4590ms  11.280us  292.33ms  cudaMallocManaged\n",
            "                   10.32%  104.22ms       210  496.28us  4.3710us  2.2203ms  cudaMemPrefetchAsync\n",
            "                    4.73%  47.784ms       120  398.20us  62.244us  6.9795ms  cudaFree\n",
            "                    0.94%  9.5268ms        60  158.78us  8.6430us  947.72us  cudaLaunchKernel\n",
            "                    0.04%  413.20us       120  3.4430us  1.6120us  18.377us  cudaMemAdvise\n",
            "                    0.02%  168.08us       101  1.6640us     173ns  71.122us  cuDeviceGetAttribute\n",
            "                    0.01%  91.749us        30  3.0580us  2.5450us  6.2000us  cudaGetDevice\n",
            "                    0.00%  38.527us         1  38.527us  38.527us  38.527us  cuDeviceGetName\n",
            "                    0.00%  11.086us         1  11.086us  11.086us  11.086us  cuDeviceGetPCIBusId\n",
            "                    0.00%  3.1410us         3  1.0470us     241ns  2.5480us  cuDeviceGetCount\n",
            "                    0.00%  1.2730us         2     636ns     294ns     979ns  cuDeviceGet\n",
            "                    0.00%     885ns         1     885ns     885ns     885ns  cuModuleGetLoadingMode\n",
            "                    0.00%     719ns         1     719ns     719ns     719ns  cuDeviceTotalMem\n",
            "                    0.00%     361ns         1     361ns     361ns     361ns  cuDeviceGetUuid\n",
            "\n",
            "==12977== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "     120  2.0000MB  2.0000MB  2.0000MB  240.0000MB  20.94814ms  Host To Device\n",
            "      30  4.0000KB  4.0000KB  4.0000KB  120.0000KB  50.23700us  Device To Host\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "nvprof ./cuda_dotproduct1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83NS8AFURwhI"
      },
      "source": [
        "#### vector size: 2^22"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3E2HYvqiRwhJ",
        "outputId": "88d8e83a-2188-454c-ad3c-572cf3b18edc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting cuda_dotproduct1.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile cuda_dotproduct1.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "\n",
        "__device__ double atomicAddDouble(double* address, double val){\n",
        "    unsigned long long int* address_as_ull = (unsigned long long int*)address;\n",
        "    unsigned long long int old = *address_as_ull, assumed;\n",
        "    do {\n",
        "        assumed = old;\n",
        "        old = atomicCAS(address_as_ull, assumed, __double_as_longlong(val + __longlong_as_double(assumed)));\n",
        "    } while (assumed != old);\n",
        "    return __longlong_as_double(old);\n",
        "}\n",
        "\n",
        "__global__\n",
        "void sumDot(int n, int numThreads, float* X, float* Y, double* dotp, double* dotp_temp){\n",
        "\n",
        "  __shared__ double sharedTemp;\n",
        "  sharedTemp = 0.0;\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "\n",
        "  for(int i = index; i < n; i += stride){\n",
        "    __syncthreads();\n",
        "    atomicAddDouble(&sharedTemp, dotp_temp[i]);\n",
        "    //printf(\"index = %d| sharedTemp = %f\\n\", index, sharedTemp);\n",
        "    __syncthreads();\n",
        "    if (index % blockDim.x == 0)\n",
        "    {\n",
        "      atomicAddDouble(dotp, sharedTemp);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__\n",
        "void dotProduct(int n, int numThreads, float* X, float* Y, double* dotp, double* dotp_temp){\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "\n",
        "  // Fill dotp_temp with the product\n",
        "  for(int i = index; i < n; i += stride){\n",
        "    dotp_temp[i] = X[i] * Y[i];\n",
        "  }\n",
        "}\n",
        "\n",
        "int bulk_logic(unsigned int array_size){\n",
        "  const unsigned int ARRAY_SIZE = array_size;\n",
        "  const unsigned int ARRAY_BYTES = ARRAY_SIZE * sizeof(float);\n",
        "\n",
        "  //declare array\n",
        "  float *X, *Y;\n",
        "  double *dotp, *dotp_temp;\n",
        "  cudaMallocManaged(&X, ARRAY_BYTES);\n",
        "  cudaMallocManaged(&Y, ARRAY_BYTES);\n",
        "  cudaMallocManaged(&dotp, sizeof(double));\n",
        "  cudaMallocManaged(&dotp_temp, ARRAY_SIZE * sizeof(double));\n",
        "\n",
        "  //prefetch data to create CPU page memory\n",
        "  int device = -1;\n",
        "  cudaGetDevice(&device);\n",
        "\n",
        "  //memory advise\n",
        "  cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId);\n",
        "\n",
        "  //\"prefetch data\" to create CPU page memory\n",
        "  cudaMemPrefetchAsync(X, ARRAY_BYTES, cudaCpuDeviceId, NULL);\n",
        "  cudaMemPrefetchAsync(Y, ARRAY_BYTES, cudaCpuDeviceId, NULL);\n",
        "\n",
        "  //\"prefetch data\" to create GPU page memory\n",
        "  cudaMemPrefetchAsync(dotp, sizeof(double), device, NULL);\n",
        "  cudaMemPrefetchAsync(dotp_temp, ARRAY_SIZE * sizeof(double), device, NULL);\n",
        "\n",
        "  //initialize array\n",
        "  for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "    X[i] = float(1);\n",
        "    Y[i] = float(1);\n",
        "  }\n",
        "\n",
        "  //(to transfer data from the) Actual transfer: CPU to GPU\n",
        "  cudaMemPrefetchAsync(X, ARRAY_BYTES, device, NULL);\n",
        "  cudaMemPrefetchAsync(Y, ARRAY_BYTES, device, NULL);\n",
        "\n",
        "  //call cuda kernel\n",
        "  int numThreads = 512;\n",
        "  int numBlocks = (ARRAY_SIZE+numThreads-1)/numThreads;\n",
        "  printf(\"numBlocks = %d, numThreads = %d\\n\", numBlocks, numThreads);\n",
        "\n",
        "  dotProduct<<<numBlocks, numThreads>>> (ARRAY_SIZE, numThreads, X, Y, dotp, dotp_temp);\n",
        "  sumDot<<<numBlocks, numThreads>>> (ARRAY_SIZE, numThreads, X, Y, dotp, dotp_temp);\n",
        "\n",
        "  //barrier\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  //prefetch\n",
        "  cudaMemPrefetchAsync(dotp, sizeof(double), cudaCpuDeviceId, NULL);\n",
        "\n",
        "  //error checking\n",
        "  double expected_dotp  =  0.0;\n",
        "  for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "    expected_dotp = expected_dotp + (X[i] * Y[i]);\n",
        "  }\n",
        "\n",
        "  printf(\"Actual Dot Product: %.2f\\n\", *dotp);\n",
        "  printf(\"Expected Dot Product: %.2f\\n\", expected_dotp);\n",
        "\n",
        "  if(*dotp != expected_dotp){\n",
        "    printf(\"ERROR!: Actual Dot Product and Expected Dot Product is NOT EQUAL\\n\");\n",
        "  }\n",
        "  else{\n",
        "    printf(\"NO ERROR!\\n\");\n",
        "  }\n",
        "\n",
        "  //free memory\n",
        "  cudaFree(X);\n",
        "  cudaFree(Y);\n",
        "  cudaFree(dotp);\n",
        "  cudaFree(dotp_temp);\n",
        "\n",
        "  return 0;\n",
        "}\n",
        "\n",
        "\n",
        "int main(){\n",
        "  int numberOfRuns = 30;\n",
        "  //############CHANGE HERE######################\n",
        "  unsigned int array_size = 1<<22;\n",
        "  //############CHANGE HERE######################\n",
        "\n",
        "  for (int i = 0; i < numberOfRuns; i++) {\n",
        "    bulk_logic(array_size);\n",
        "  }\n",
        "return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pXyp9LNRwhJ",
        "outputId": "6dcd6beb-f9ef-4fee-f6ef-d6b1de750b54"
      },
      "outputs": [
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "nvcc cuda_dotproduct1.cu -o cuda_dotproduct1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-q3E-6yRwhJ",
        "outputId": "ed347110-c61d-4481-a04c-973542a2296c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==13264== NVPROF is profiling process 13264, command: ./cuda_dotproduct1\n",
            "numBlocks = 8192, numThreads = 512\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 8192, numThreads = 512\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 8192, numThreads = 512\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 8192, numThreads = 512\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 8192, numThreads = 512\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 8192, numThreads = 512\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 8192, numThreads = 512\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 8192, numThreads = 512\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 8192, numThreads = 512\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 8192, numThreads = 512\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 8192, numThreads = 512\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 8192, numThreads = 512\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 8192, numThreads = 512\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 8192, numThreads = 512\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 8192, numThreads = 512\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 8192, numThreads = 512\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 8192, numThreads = 512\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 8192, numThreads = 512\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 8192, numThreads = 512\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 8192, numThreads = 512\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 8192, numThreads = 512\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 8192, numThreads = 512\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 8192, numThreads = 512\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 8192, numThreads = 512\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 8192, numThreads = 512\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 8192, numThreads = 512\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 8192, numThreads = 512\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 8192, numThreads = 512\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 8192, numThreads = 512\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 8192, numThreads = 512\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "==13264== Profiling application: ./cuda_dotproduct1\n",
            "==13264== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.52%  1.67784s        30  55.928ms  51.301ms  139.91ms  sumDot(int, int, float*, float*, double*, double*)\n",
            "                    0.48%  8.1201ms        30  270.67us  266.52us  283.04us  dotProduct(int, int, float*, float*, double*, double*)\n",
            "      API calls:   66.75%  1.69719s        30  56.573ms  51.559ms  140.19ms  cudaDeviceSynchronize\n",
            "                   18.28%  464.88ms       210  2.2137ms  12.653us  8.5485ms  cudaMemPrefetchAsync\n",
            "                   10.12%  257.30ms       120  2.1442ms  8.8570us  254.58ms  cudaMallocManaged\n",
            "                    4.74%  120.47ms       120  1.0039ms  62.142us  4.0832ms  cudaFree\n",
            "                    0.08%  1.9782ms        60  32.970us  9.2650us  86.399us  cudaLaunchKernel\n",
            "                    0.01%  353.63us       120  2.9460us  1.0670us  11.784us  cudaMemAdvise\n",
            "                    0.01%  136.81us       101  1.3540us     127ns  54.782us  cuDeviceGetAttribute\n",
            "                    0.00%  86.077us        30  2.8690us  2.2120us  3.5280us  cudaGetDevice\n",
            "                    0.00%  26.773us         1  26.773us  26.773us  26.773us  cuDeviceGetName\n",
            "                    0.00%  6.9370us         1  6.9370us  6.9370us  6.9370us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.8910us         3     630ns     210ns  1.4270us  cuDeviceGetCount\n",
            "                    0.00%  1.0010us         2     500ns     218ns     783ns  cuDeviceGet\n",
            "                    0.00%     768ns         1     768ns     768ns     768ns  cuDeviceTotalMem\n",
            "                    0.00%     481ns         1     481ns     481ns     481ns  cuModuleGetLoadingMode\n",
            "                    0.00%     403ns         1     403ns     403ns     403ns  cuDeviceGetUuid\n",
            "\n",
            "==13264== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "     480  2.0000MB  2.0000MB  2.0000MB  960.0000MB  85.11538ms  Host To Device\n",
            "      30  4.0000KB  4.0000KB  4.0000KB  120.0000KB  47.35900us  Device To Host\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "nvprof ./cuda_dotproduct1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZBdoNebRwhJ"
      },
      "source": [
        "#### vector size: 2^24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qDIpAFCRwhK",
        "outputId": "622dd409-36f2-4066-ff14-593108cbd2b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting cuda_dotproduct1.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile cuda_dotproduct1.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "\n",
        "__device__ double atomicAddDouble(double* address, double val){\n",
        "    unsigned long long int* address_as_ull = (unsigned long long int*)address;\n",
        "    unsigned long long int old = *address_as_ull, assumed;\n",
        "    do {\n",
        "        assumed = old;\n",
        "        old = atomicCAS(address_as_ull, assumed, __double_as_longlong(val + __longlong_as_double(assumed)));\n",
        "    } while (assumed != old);\n",
        "    return __longlong_as_double(old);\n",
        "}\n",
        "\n",
        "__global__\n",
        "void sumDot(int n, int numThreads, float* X, float* Y, double* dotp, double* dotp_temp){\n",
        "\n",
        "  __shared__ double sharedTemp;\n",
        "  sharedTemp = 0.0;\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "\n",
        "  for(int i = index; i < n; i += stride){\n",
        "    __syncthreads();\n",
        "    atomicAddDouble(&sharedTemp, dotp_temp[i]);\n",
        "    //printf(\"index = %d| sharedTemp = %f\\n\", index, sharedTemp);\n",
        "    __syncthreads();\n",
        "    if (index % blockDim.x == 0)\n",
        "    {\n",
        "      atomicAddDouble(dotp, sharedTemp);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__\n",
        "void dotProduct(int n, int numThreads, float* X, float* Y, double* dotp, double* dotp_temp){\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "\n",
        "  // Fill dotp_temp with the product\n",
        "  for(int i = index; i < n; i += stride){\n",
        "    dotp_temp[i] = X[i] * Y[i];\n",
        "  }\n",
        "}\n",
        "\n",
        "int bulk_logic(unsigned int array_size){\n",
        "  const unsigned int ARRAY_SIZE = array_size;\n",
        "  const unsigned int ARRAY_BYTES = ARRAY_SIZE * sizeof(float);\n",
        "\n",
        "  //declare array\n",
        "  float *X, *Y;\n",
        "  double *dotp, *dotp_temp;\n",
        "  cudaMallocManaged(&X, ARRAY_BYTES);\n",
        "  cudaMallocManaged(&Y, ARRAY_BYTES);\n",
        "  cudaMallocManaged(&dotp, sizeof(double));\n",
        "  cudaMallocManaged(&dotp_temp, ARRAY_SIZE * sizeof(double));\n",
        "\n",
        "  //prefetch data to create CPU page memory\n",
        "  int device = -1;\n",
        "  cudaGetDevice(&device);\n",
        "\n",
        "  //memory advise\n",
        "  cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId);\n",
        "\n",
        "  //\"prefetch data\" to create CPU page memory\n",
        "  cudaMemPrefetchAsync(X, ARRAY_BYTES, cudaCpuDeviceId, NULL);\n",
        "  cudaMemPrefetchAsync(Y, ARRAY_BYTES, cudaCpuDeviceId, NULL);\n",
        "\n",
        "  //\"prefetch data\" to create GPU page memory\n",
        "  cudaMemPrefetchAsync(dotp, sizeof(double), device, NULL);\n",
        "  cudaMemPrefetchAsync(dotp_temp, ARRAY_SIZE * sizeof(double), device, NULL);\n",
        "\n",
        "  //initialize array\n",
        "  for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "    X[i] = float(1);\n",
        "    Y[i] = float(1);\n",
        "  }\n",
        "\n",
        "  //(to transfer data from the) Actual transfer: CPU to GPU\n",
        "  cudaMemPrefetchAsync(X, ARRAY_BYTES, device, NULL);\n",
        "  cudaMemPrefetchAsync(Y, ARRAY_BYTES, device, NULL);\n",
        "\n",
        "  //call cuda kernel\n",
        "  int numThreads = 512;\n",
        "  int numBlocks = (ARRAY_SIZE+numThreads-1)/numThreads;\n",
        "  printf(\"numBlocks = %d, numThreads = %d\\n\", numBlocks, numThreads);\n",
        "\n",
        "  dotProduct<<<numBlocks, numThreads>>> (ARRAY_SIZE, numThreads, X, Y, dotp, dotp_temp);\n",
        "  sumDot<<<numBlocks, numThreads>>> (ARRAY_SIZE, numThreads, X, Y, dotp, dotp_temp);\n",
        "\n",
        "  //barrier\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  //prefetch\n",
        "  cudaMemPrefetchAsync(dotp, sizeof(double), cudaCpuDeviceId, NULL);\n",
        "\n",
        "  //error checking\n",
        "  double expected_dotp  =  0.0;\n",
        "  for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "    expected_dotp = expected_dotp + (X[i] * Y[i]);\n",
        "  }\n",
        "\n",
        "  printf(\"Actual Dot Product: %.2f\\n\", *dotp);\n",
        "  printf(\"Expected Dot Product: %.2f\\n\", expected_dotp);\n",
        "\n",
        "  if(*dotp != expected_dotp){\n",
        "    printf(\"ERROR!: Actual Dot Product and Expected Dot Product is NOT EQUAL\\n\");\n",
        "  }\n",
        "  else{\n",
        "    printf(\"NO ERROR!\\n\");\n",
        "  }\n",
        "\n",
        "  //free memory\n",
        "  cudaFree(X);\n",
        "  cudaFree(Y);\n",
        "  cudaFree(dotp);\n",
        "  cudaFree(dotp_temp);\n",
        "\n",
        "  return 0;\n",
        "}\n",
        "\n",
        "\n",
        "int main(){\n",
        "  int numberOfRuns = 30;\n",
        "  //############CHANGE HERE######################\n",
        "  unsigned int array_size = 1<<24;\n",
        "  //############CHANGE HERE######################\n",
        "\n",
        "  for (int i = 0; i < numberOfRuns; i++) {\n",
        "    bulk_logic(array_size);\n",
        "  }\n",
        "return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyFx_KgSRwhK",
        "outputId": "601c3170-3f67-49e8-c2ab-682f4626168b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "nvcc cuda_dotproduct1.cu -o cuda_dotproduct1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4vesfa8RwhK",
        "outputId": "0ed54d67-e1bf-46c2-8137-6148c9bda627"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==13709== NVPROF is profiling process 13709, command: ./cuda_dotproduct1\n",
            "numBlocks = 32768, numThreads = 512\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 32768, numThreads = 512\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 32768, numThreads = 512\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 32768, numThreads = 512\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 32768, numThreads = 512\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 32768, numThreads = 512\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 32768, numThreads = 512\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 32768, numThreads = 512\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 32768, numThreads = 512\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 32768, numThreads = 512\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 32768, numThreads = 512\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 32768, numThreads = 512\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 32768, numThreads = 512\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 32768, numThreads = 512\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 32768, numThreads = 512\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 32768, numThreads = 512\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 32768, numThreads = 512\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 32768, numThreads = 512\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 32768, numThreads = 512\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 32768, numThreads = 512\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 32768, numThreads = 512\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 32768, numThreads = 512\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 32768, numThreads = 512\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 32768, numThreads = 512\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 32768, numThreads = 512\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 32768, numThreads = 512\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 32768, numThreads = 512\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 32768, numThreads = 512\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 32768, numThreads = 512\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 32768, numThreads = 512\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "==13709== Profiling application: ./cuda_dotproduct1\n",
            "==13709== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.48%  6.30952s        30  210.32ms  204.55ms  373.45ms  sumDot(int, int, float*, float*, double*, double*)\n",
            "                    0.52%  33.044ms        30  1.1015ms  1.0979ms  1.1245ms  dotProduct(int, int, float*, float*, double*, double*)\n",
            "      API calls:   72.91%  6.34279s        30  211.43ms  205.66ms  374.57ms  cudaDeviceSynchronize\n",
            "                   19.50%  1.69617s       210  8.0770ms  47.330us  24.778ms  cudaMemPrefetchAsync\n",
            "                    4.68%  407.24ms       120  3.3936ms  66.485us  8.9006ms  cudaFree\n",
            "                    2.88%  250.20ms       120  2.0850ms  11.505us  247.03ms  cudaMallocManaged\n",
            "                    0.02%  1.9244ms        60  32.074us  8.1210us  73.082us  cudaLaunchKernel\n",
            "                    0.00%  354.19us       120  2.9510us  1.0630us  14.373us  cudaMemAdvise\n",
            "                    0.00%  125.48us       101  1.2420us     137ns  52.170us  cuDeviceGetAttribute\n",
            "                    0.00%  80.753us        30  2.6910us  2.2240us  3.3330us  cudaGetDevice\n",
            "                    0.00%  28.912us         1  28.912us  28.912us  28.912us  cuDeviceGetName\n",
            "                    0.00%  14.460us         1  14.460us  14.460us  14.460us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.9770us         3     659ns     207ns  1.5090us  cuDeviceGetCount\n",
            "                    0.00%     954ns         2     477ns     274ns     680ns  cuDeviceGet\n",
            "                    0.00%     465ns         1     465ns     465ns     465ns  cuDeviceTotalMem\n",
            "                    0.00%     378ns         1     378ns     378ns     378ns  cuModuleGetLoadingMode\n",
            "                    0.00%     226ns         1     226ns     226ns     226ns  cuDeviceGetUuid\n",
            "\n",
            "==13709== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "    1920  2.0000MB  2.0000MB  2.0000MB  3.750000GB  338.2091ms  Host To Device\n",
            "      30  4.0000KB  4.0000KB  4.0000KB  120.0000KB  46.17400us  Device To Host\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "nvprof ./cuda_dotproduct1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPa337uISN85"
      },
      "source": [
        "### 1024 threads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXYnMT5BSN9A"
      },
      "source": [
        "#### vector size: 2^20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXs3rRsDSN9A",
        "outputId": "29471c90-3480-4482-f45d-ca0982dcb83e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing cuda_dotproduct1.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile cuda_dotproduct1.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "\n",
        "__device__ double atomicAddDouble(double* address, double val){\n",
        "    unsigned long long int* address_as_ull = (unsigned long long int*)address;\n",
        "    unsigned long long int old = *address_as_ull, assumed;\n",
        "    do {\n",
        "        assumed = old;\n",
        "        old = atomicCAS(address_as_ull, assumed, __double_as_longlong(val + __longlong_as_double(assumed)));\n",
        "    } while (assumed != old);\n",
        "    return __longlong_as_double(old);\n",
        "}\n",
        "\n",
        "__global__\n",
        "void sumDot(int n, int numThreads, float* X, float* Y, double* dotp, double* dotp_temp){\n",
        "\n",
        "  __shared__ double sharedTemp;\n",
        "  sharedTemp = 0.0;\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "\n",
        "  for(int i = index; i < n; i += stride){\n",
        "    __syncthreads();\n",
        "    atomicAddDouble(&sharedTemp, dotp_temp[i]);\n",
        "    //printf(\"index = %d| sharedTemp = %f\\n\", index, sharedTemp);\n",
        "    __syncthreads();\n",
        "    if (index % blockDim.x == 0)\n",
        "    {\n",
        "      atomicAddDouble(dotp, sharedTemp);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__\n",
        "void dotProduct(int n, int numThreads, float* X, float* Y, double* dotp, double* dotp_temp){\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "\n",
        "  // Fill dotp_temp with the product\n",
        "  for(int i = index; i < n; i += stride){\n",
        "    dotp_temp[i] = X[i] * Y[i];\n",
        "  }\n",
        "}\n",
        "\n",
        "int bulk_logic(unsigned int array_size){\n",
        "  const unsigned int ARRAY_SIZE = array_size;\n",
        "  const unsigned int ARRAY_BYTES = ARRAY_SIZE * sizeof(float);\n",
        "\n",
        "  //declare array\n",
        "  float *X, *Y;\n",
        "  double *dotp, *dotp_temp;\n",
        "  cudaMallocManaged(&X, ARRAY_BYTES);\n",
        "  cudaMallocManaged(&Y, ARRAY_BYTES);\n",
        "  cudaMallocManaged(&dotp, sizeof(double));\n",
        "  cudaMallocManaged(&dotp_temp, ARRAY_SIZE * sizeof(double));\n",
        "\n",
        "  //prefetch data to create CPU page memory\n",
        "  int device = -1;\n",
        "  cudaGetDevice(&device);\n",
        "\n",
        "  //memory advise\n",
        "  cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId);\n",
        "\n",
        "  //\"prefetch data\" to create CPU page memory\n",
        "  cudaMemPrefetchAsync(X, ARRAY_BYTES, cudaCpuDeviceId, NULL);\n",
        "  cudaMemPrefetchAsync(Y, ARRAY_BYTES, cudaCpuDeviceId, NULL);\n",
        "\n",
        "  //\"prefetch data\" to create GPU page memory\n",
        "  cudaMemPrefetchAsync(dotp, sizeof(double), device, NULL);\n",
        "  cudaMemPrefetchAsync(dotp_temp, ARRAY_SIZE * sizeof(double), device, NULL);\n",
        "\n",
        "  //initialize array\n",
        "  for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "    X[i] = float(1);\n",
        "    Y[i] = float(1);\n",
        "  }\n",
        "\n",
        "  //(to transfer data from the) Actual transfer: CPU to GPU\n",
        "  cudaMemPrefetchAsync(X, ARRAY_BYTES, device, NULL);\n",
        "  cudaMemPrefetchAsync(Y, ARRAY_BYTES, device, NULL);\n",
        "\n",
        "  //call cuda kernel\n",
        "  int numThreads = 1024;\n",
        "  int numBlocks = (ARRAY_SIZE+numThreads-1)/numThreads;\n",
        "  printf(\"numBlocks = %d, numThreads = %d\\n\", numBlocks, numThreads);\n",
        "\n",
        "  dotProduct<<<numBlocks, numThreads>>> (ARRAY_SIZE, numThreads, X, Y, dotp, dotp_temp);\n",
        "  sumDot<<<numBlocks, numThreads>>> (ARRAY_SIZE, numThreads, X, Y, dotp, dotp_temp);\n",
        "\n",
        "  //barrier\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  //prefetch\n",
        "  cudaMemPrefetchAsync(dotp, sizeof(double), cudaCpuDeviceId, NULL);\n",
        "\n",
        "  //error checking\n",
        "  double expected_dotp  =  0.0;\n",
        "  for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "    expected_dotp = expected_dotp + (X[i] * Y[i]);\n",
        "  }\n",
        "\n",
        "  printf(\"Actual Dot Product: %.2f\\n\", *dotp);\n",
        "  printf(\"Expected Dot Product: %.2f\\n\", expected_dotp);\n",
        "\n",
        "  if(*dotp != expected_dotp){\n",
        "    printf(\"ERROR!: Actual Dot Product and Expected Dot Product is NOT EQUAL\\n\");\n",
        "  }\n",
        "  else{\n",
        "    printf(\"NO ERROR!\\n\");\n",
        "  }\n",
        "\n",
        "  //free memory\n",
        "  cudaFree(X);\n",
        "  cudaFree(Y);\n",
        "  cudaFree(dotp);\n",
        "  cudaFree(dotp_temp);\n",
        "\n",
        "  return 0;\n",
        "}\n",
        "\n",
        "\n",
        "int main(){\n",
        "  int numberOfRuns = 30;\n",
        "  //############CHANGE HERE######################\n",
        "  unsigned int array_size = 1<<20;\n",
        "  //############CHANGE HERE######################\n",
        "\n",
        "  for (int i = 0; i < numberOfRuns; i++) {\n",
        "    bulk_logic(array_size);\n",
        "  }\n",
        "return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEHr6O9fSN9B",
        "outputId": "a4a82758-d765-4a21-8e3e-a0cd2563d19f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "nvcc cuda_dotproduct1.cu -o cuda_dotproduct1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsslSuLSSN9B",
        "outputId": "730c9fa3-2ff2-46e8-c463-05dc8f683042"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==694== NVPROF is profiling process 694, command: ./cuda_dotproduct1\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "==694== Profiling application: ./cuda_dotproduct1\n",
            "==694== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.80%  1.00456s        30  33.485ms  28.440ms  78.142ms  sumDot(int, int, float*, float*, double*, double*)\n",
            "                    0.20%  1.9854ms        30  66.180us  63.487us  90.687us  dotProduct(int, int, float*, float*, double*, double*)\n",
            "      API calls:   65.93%  1.04030s        30  34.677ms  29.517ms  79.252ms  cudaDeviceSynchronize\n",
            "                   28.97%  457.17ms       120  3.8098ms  7.7540us  455.47ms  cudaMallocManaged\n",
            "                    3.11%  49.068ms       210  233.66us  8.5300us  1.1447ms  cudaMemPrefetchAsync\n",
            "                    1.51%  23.851ms       120  198.76us  40.365us  767.39us  cudaFree\n",
            "                    0.38%  6.0350ms         1  6.0350ms  6.0350ms  6.0350ms  cuDeviceGetPCIBusId\n",
            "                    0.06%  977.19us        60  16.286us  6.9290us  47.838us  cudaLaunchKernel\n",
            "                    0.02%  247.19us       120  2.0590us     963ns  14.764us  cudaMemAdvise\n",
            "                    0.01%  165.04us       101  1.6340us     202ns  64.971us  cuDeviceGetAttribute\n",
            "                    0.00%  36.750us        30  1.2250us     893ns  2.4380us  cudaGetDevice\n",
            "                    0.00%  27.692us         1  27.692us  27.692us  27.692us  cuDeviceGetName\n",
            "                    0.00%  2.2670us         3     755ns     320ns  1.5770us  cuDeviceGetCount\n",
            "                    0.00%  1.4700us         2     735ns     271ns  1.1990us  cuDeviceGet\n",
            "                    0.00%     660ns         1     660ns     660ns     660ns  cuDeviceTotalMem\n",
            "                    0.00%     511ns         1     511ns     511ns     511ns  cuModuleGetLoadingMode\n",
            "                    0.00%     346ns         1     346ns     346ns     346ns  cuDeviceGetUuid\n",
            "\n",
            "==694== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "     120  2.0000MB  2.0000MB  2.0000MB  240.0000MB  20.95752ms  Host To Device\n",
            "      30  4.0000KB  4.0000KB  4.0000KB  120.0000KB  46.55900us  Device To Host\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "nvprof ./cuda_dotproduct1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5c7J1pBSN9B"
      },
      "source": [
        "#### vector size: 2^22"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGGYe3_oSN9B",
        "outputId": "d2445d29-aa4f-408a-fbe8-1a80b91e35fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting cuda_dotproduct1.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile cuda_dotproduct1.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "\n",
        "__device__ double atomicAddDouble(double* address, double val){\n",
        "    unsigned long long int* address_as_ull = (unsigned long long int*)address;\n",
        "    unsigned long long int old = *address_as_ull, assumed;\n",
        "    do {\n",
        "        assumed = old;\n",
        "        old = atomicCAS(address_as_ull, assumed, __double_as_longlong(val + __longlong_as_double(assumed)));\n",
        "    } while (assumed != old);\n",
        "    return __longlong_as_double(old);\n",
        "}\n",
        "\n",
        "__global__\n",
        "void sumDot(int n, int numThreads, float* X, float* Y, double* dotp, double* dotp_temp){\n",
        "\n",
        "  __shared__ double sharedTemp;\n",
        "  sharedTemp = 0.0;\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "\n",
        "  for(int i = index; i < n; i += stride){\n",
        "    __syncthreads();\n",
        "    atomicAddDouble(&sharedTemp, dotp_temp[i]);\n",
        "    //printf(\"index = %d| sharedTemp = %f\\n\", index, sharedTemp);\n",
        "    __syncthreads();\n",
        "    if (index % blockDim.x == 0)\n",
        "    {\n",
        "      atomicAddDouble(dotp, sharedTemp);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__\n",
        "void dotProduct(int n, int numThreads, float* X, float* Y, double* dotp, double* dotp_temp){\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "\n",
        "  // Fill dotp_temp with the product\n",
        "  for(int i = index; i < n; i += stride){\n",
        "    dotp_temp[i] = X[i] * Y[i];\n",
        "  }\n",
        "}\n",
        "\n",
        "int bulk_logic(unsigned int array_size){\n",
        "  const unsigned int ARRAY_SIZE = array_size;\n",
        "  const unsigned int ARRAY_BYTES = ARRAY_SIZE * sizeof(float);\n",
        "\n",
        "  //declare array\n",
        "  float *X, *Y;\n",
        "  double *dotp, *dotp_temp;\n",
        "  cudaMallocManaged(&X, ARRAY_BYTES);\n",
        "  cudaMallocManaged(&Y, ARRAY_BYTES);\n",
        "  cudaMallocManaged(&dotp, sizeof(double));\n",
        "  cudaMallocManaged(&dotp_temp, ARRAY_SIZE * sizeof(double));\n",
        "\n",
        "  //prefetch data to create CPU page memory\n",
        "  int device = -1;\n",
        "  cudaGetDevice(&device);\n",
        "\n",
        "  //memory advise\n",
        "  cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId);\n",
        "\n",
        "  //\"prefetch data\" to create CPU page memory\n",
        "  cudaMemPrefetchAsync(X, ARRAY_BYTES, cudaCpuDeviceId, NULL);\n",
        "  cudaMemPrefetchAsync(Y, ARRAY_BYTES, cudaCpuDeviceId, NULL);\n",
        "\n",
        "  //\"prefetch data\" to create GPU page memory\n",
        "  cudaMemPrefetchAsync(dotp, sizeof(double), device, NULL);\n",
        "  cudaMemPrefetchAsync(dotp_temp, ARRAY_SIZE * sizeof(double), device, NULL);\n",
        "\n",
        "  //initialize array\n",
        "  for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "    X[i] = float(1);\n",
        "    Y[i] = float(1);\n",
        "  }\n",
        "\n",
        "  //(to transfer data from the) Actual transfer: CPU to GPU\n",
        "  cudaMemPrefetchAsync(X, ARRAY_BYTES, device, NULL);\n",
        "  cudaMemPrefetchAsync(Y, ARRAY_BYTES, device, NULL);\n",
        "\n",
        "  //call cuda kernel\n",
        "  int numThreads = 1024;\n",
        "  int numBlocks = (ARRAY_SIZE+numThreads-1)/numThreads;\n",
        "  printf(\"numBlocks = %d, numThreads = %d\\n\", numBlocks, numThreads);\n",
        "\n",
        "  dotProduct<<<numBlocks, numThreads>>> (ARRAY_SIZE, numThreads, X, Y, dotp, dotp_temp);\n",
        "  sumDot<<<numBlocks, numThreads>>> (ARRAY_SIZE, numThreads, X, Y, dotp, dotp_temp);\n",
        "\n",
        "  //barrier\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  //prefetch\n",
        "  cudaMemPrefetchAsync(dotp, sizeof(double), cudaCpuDeviceId, NULL);\n",
        "\n",
        "  //error checking\n",
        "  double expected_dotp  =  0.0;\n",
        "  for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "    expected_dotp = expected_dotp + (X[i] * Y[i]);\n",
        "  }\n",
        "\n",
        "  printf(\"Actual Dot Product: %.2f\\n\", *dotp);\n",
        "  printf(\"Expected Dot Product: %.2f\\n\", expected_dotp);\n",
        "\n",
        "  if(*dotp != expected_dotp){\n",
        "    printf(\"ERROR!: Actual Dot Product and Expected Dot Product is NOT EQUAL\\n\");\n",
        "  }\n",
        "  else{\n",
        "    printf(\"NO ERROR!\\n\");\n",
        "  }\n",
        "\n",
        "  //free memory\n",
        "  cudaFree(X);\n",
        "  cudaFree(Y);\n",
        "  cudaFree(dotp);\n",
        "  cudaFree(dotp_temp);\n",
        "\n",
        "  return 0;\n",
        "}\n",
        "\n",
        "\n",
        "int main(){\n",
        "  int numberOfRuns = 30;\n",
        "  //############CHANGE HERE######################\n",
        "  unsigned int array_size = 1<<22;\n",
        "  //############CHANGE HERE######################\n",
        "\n",
        "  for (int i = 0; i < numberOfRuns; i++) {\n",
        "    bulk_logic(array_size);\n",
        "  }\n",
        "return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0F1eQHp3SN9C",
        "outputId": "0af15b65-48d3-40d5-d78d-185288b4f18d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "nvcc cuda_dotproduct1.cu -o cuda_dotproduct1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxT11xXuSN9C",
        "outputId": "6147108a-026d-4d2d-ca52-bbd6c48273d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==945== NVPROF is profiling process 945, command: ./cuda_dotproduct1\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "==945== Profiling application: ./cuda_dotproduct1\n",
            "==945== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.77%  3.56338s        30  118.78ms  113.76ms  240.76ms  sumDot(int, int, float*, float*, double*, double*)\n",
            "                    0.23%  8.2098ms        30  273.66us  270.62us  346.08us  dotProduct(int, int, float*, float*, double*, double*)\n",
            "      API calls:   82.89%  3.60397s        30  120.13ms  114.03ms  241.10ms  cudaDeviceSynchronize\n",
            "                    8.81%  382.95ms       210  1.8236ms  10.797us  9.0559ms  cudaMemPrefetchAsync\n",
            "                    5.41%  235.26ms       120  1.9605ms  8.7670us  232.51ms  cudaMallocManaged\n",
            "                    2.84%  123.53ms       120  1.0295ms  56.756us  3.3390ms  cudaFree\n",
            "                    0.04%  1.7807ms        60  29.678us  7.9040us  78.475us  cudaLaunchKernel\n",
            "                    0.01%  303.11us       120  2.5250us     964ns  12.483us  cudaMemAdvise\n",
            "                    0.00%  113.17us       101  1.1200us     128ns  47.066us  cuDeviceGetAttribute\n",
            "                    0.00%  70.348us        30  2.3440us  1.6040us  2.9720us  cudaGetDevice\n",
            "                    0.00%  28.182us         1  28.182us  28.182us  28.182us  cuDeviceGetName\n",
            "                    0.00%  5.7210us         1  5.7210us  5.7210us  5.7210us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.3060us         3     768ns     253ns  1.7230us  cuDeviceGetCount\n",
            "                    0.00%     807ns         2     403ns     257ns     550ns  cuDeviceGet\n",
            "                    0.00%     662ns         1     662ns     662ns     662ns  cuDeviceTotalMem\n",
            "                    0.00%     344ns         1     344ns     344ns     344ns  cuModuleGetLoadingMode\n",
            "                    0.00%     205ns         1     205ns     205ns     205ns  cuDeviceGetUuid\n",
            "\n",
            "==945== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "     480  2.0000MB  2.0000MB  2.0000MB  960.0000MB  83.74764ms  Host To Device\n",
            "      30  4.0000KB  4.0000KB  4.0000KB  120.0000KB  44.77000us  Device To Host\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "nvprof ./cuda_dotproduct1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_EJIIegSN9C"
      },
      "source": [
        "#### vector size: 2^24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5s0sFzlSN9C",
        "outputId": "46165170-04fa-4227-ba80-da2d81f58c5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting cuda_dotproduct1.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile cuda_dotproduct1.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "\n",
        "__device__ double atomicAddDouble(double* address, double val){\n",
        "    unsigned long long int* address_as_ull = (unsigned long long int*)address;\n",
        "    unsigned long long int old = *address_as_ull, assumed;\n",
        "    do {\n",
        "        assumed = old;\n",
        "        old = atomicCAS(address_as_ull, assumed, __double_as_longlong(val + __longlong_as_double(assumed)));\n",
        "    } while (assumed != old);\n",
        "    return __longlong_as_double(old);\n",
        "}\n",
        "\n",
        "__global__\n",
        "void sumDot(int n, int numThreads, float* X, float* Y, double* dotp, double* dotp_temp){\n",
        "\n",
        "  __shared__ double sharedTemp;\n",
        "  sharedTemp = 0.0;\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "\n",
        "  for(int i = index; i < n; i += stride){\n",
        "    __syncthreads();\n",
        "    atomicAddDouble(&sharedTemp, dotp_temp[i]);\n",
        "    //printf(\"index = %d| sharedTemp = %f\\n\", index, sharedTemp);\n",
        "    __syncthreads();\n",
        "    if (index % blockDim.x == 0)\n",
        "    {\n",
        "      atomicAddDouble(dotp, sharedTemp);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__\n",
        "void dotProduct(int n, int numThreads, float* X, float* Y, double* dotp, double* dotp_temp){\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "\n",
        "  // Fill dotp_temp with the product\n",
        "  for(int i = index; i < n; i += stride){\n",
        "    dotp_temp[i] = X[i] * Y[i];\n",
        "  }\n",
        "}\n",
        "\n",
        "int bulk_logic(unsigned int array_size){\n",
        "  const unsigned int ARRAY_SIZE = array_size;\n",
        "  const unsigned int ARRAY_BYTES = ARRAY_SIZE * sizeof(float);\n",
        "\n",
        "  //declare array\n",
        "  float *X, *Y;\n",
        "  double *dotp, *dotp_temp;\n",
        "  cudaMallocManaged(&X, ARRAY_BYTES);\n",
        "  cudaMallocManaged(&Y, ARRAY_BYTES);\n",
        "  cudaMallocManaged(&dotp, sizeof(double));\n",
        "  cudaMallocManaged(&dotp_temp, ARRAY_SIZE * sizeof(double));\n",
        "\n",
        "  //prefetch data to create CPU page memory\n",
        "  int device = -1;\n",
        "  cudaGetDevice(&device);\n",
        "\n",
        "  //memory advise\n",
        "  cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId);\n",
        "\n",
        "  //\"prefetch data\" to create CPU page memory\n",
        "  cudaMemPrefetchAsync(X, ARRAY_BYTES, cudaCpuDeviceId, NULL);\n",
        "  cudaMemPrefetchAsync(Y, ARRAY_BYTES, cudaCpuDeviceId, NULL);\n",
        "\n",
        "  //\"prefetch data\" to create GPU page memory\n",
        "  cudaMemPrefetchAsync(dotp, sizeof(double), device, NULL);\n",
        "  cudaMemPrefetchAsync(dotp_temp, ARRAY_SIZE * sizeof(double), device, NULL);\n",
        "\n",
        "  //initialize array\n",
        "  for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "    X[i] = float(i+1);\n",
        "    Y[i] = float(i+1);\n",
        "  }\n",
        "\n",
        "  //(to transfer data from the) Actual transfer: CPU to GPU\n",
        "  cudaMemPrefetchAsync(X, ARRAY_BYTES, device, NULL);\n",
        "  cudaMemPrefetchAsync(Y, ARRAY_BYTES, device, NULL);\n",
        "\n",
        "  //call cuda kernel\n",
        "  int numThreads = 1024;\n",
        "  int numBlocks = (ARRAY_SIZE+numThreads-1)/numThreads;\n",
        "  printf(\"numBlocks = %d, numThreads = %d\\n\", numBlocks, numThreads);\n",
        "\n",
        "  dotProduct<<<numBlocks, numThreads>>> (ARRAY_SIZE, numThreads, X, Y, dotp, dotp_temp);\n",
        "  sumDot<<<numBlocks, numThreads>>> (ARRAY_SIZE, numThreads, X, Y, dotp, dotp_temp);\n",
        "\n",
        "  //barrier\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  //prefetch\n",
        "  cudaMemPrefetchAsync(dotp, sizeof(double), cudaCpuDeviceId, NULL);\n",
        "\n",
        "  //error checking\n",
        "  double expected_dotp  =  0.0;\n",
        "  for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "    expected_dotp = expected_dotp + (X[i] * Y[i]);\n",
        "  }\n",
        "\n",
        "  printf(\"Actual Dot Product: %.2f\\n\", *dotp);\n",
        "  printf(\"Expected Dot Product: %.2f\\n\", expected_dotp);\n",
        "\n",
        "  if(*dotp != expected_dotp){\n",
        "    printf(\"ERROR!: Actual Dot Product and Expected Dot Product is NOT EQUAL\\n\");\n",
        "  }\n",
        "  else{\n",
        "    printf(\"NO ERROR!\\n\");\n",
        "  }\n",
        "\n",
        "  //free memory\n",
        "  cudaFree(X);\n",
        "  cudaFree(Y);\n",
        "  cudaFree(dotp);\n",
        "  cudaFree(dotp_temp);\n",
        "\n",
        "  return 0;\n",
        "}\n",
        "\n",
        "\n",
        "int main(){\n",
        "  int numberOfRuns = 30;\n",
        "  //############CHANGE HERE######################\n",
        "  unsigned int array_size = 1<<24;\n",
        "  //############CHANGE HERE######################\n",
        "\n",
        "  for (int i = 0; i < numberOfRuns; i++) {\n",
        "    bulk_logic(array_size);\n",
        "  }\n",
        "return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AHAQxyjSN9C",
        "outputId": "89894df0-ad39-4710-9734-f292f4e1560c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "nvcc cuda_dotproduct1.cu -o cuda_dotproduct1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obbIygIlSN9D",
        "outputId": "ddd69b9f-edc4-4bd4-d73b-7ffd50b7b450"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==540== NVPROF is profiling process 540, command: ./cuda_dotproduct1\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 65536, numThreads = 256\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "==540== Profiling application: ./cuda_dotproduct1\n",
            "==540== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.02%  3.30416s        30  110.14ms  105.03ms  245.04ms  sumDot(int, int, float*, float*, double*, double*)\n",
            "                    0.98%  32.826ms        30  1.0942ms  1.0703ms  1.0989ms  dotProduct(int, int, float*, float*, double*, double*)\n",
            "      API calls:   55.98%  3.33712s        30  111.24ms  106.12ms  246.11ms  cudaDeviceSynchronize\n",
            "                   28.85%  1.71970s       210  8.1891ms  41.748us  48.940ms  cudaMemPrefetchAsync\n",
            "                    8.22%  490.20ms       120  4.0850ms  12.020us  485.63ms  cudaMallocManaged\n",
            "                    6.88%  410.18ms       120  3.4182ms  63.177us  13.741ms  cudaFree\n",
            "                    0.03%  1.8259ms        60  30.431us  8.0890us  73.373us  cudaLaunchKernel\n",
            "                    0.02%  1.0365ms         1  1.0365ms  1.0365ms  1.0365ms  cuDeviceGetPCIBusId\n",
            "                    0.01%  418.72us       120  3.4890us  1.0020us  89.673us  cudaMemAdvise\n",
            "                    0.00%  181.25us       101  1.7940us     258ns  66.470us  cuDeviceGetAttribute\n",
            "                    0.00%  85.305us        30  2.8430us  2.3010us  4.2490us  cudaGetDevice\n",
            "                    0.00%  36.438us         1  36.438us  36.438us  36.438us  cuDeviceGetName\n",
            "                    0.00%  2.6210us         3     873ns     322ns  1.9750us  cuDeviceGetCount\n",
            "                    0.00%  1.0120us         2     506ns     352ns     660ns  cuDeviceGet\n",
            "                    0.00%     901ns         1     901ns     901ns     901ns  cuModuleGetLoadingMode\n",
            "                    0.00%     746ns         1     746ns     746ns     746ns  cuDeviceTotalMem\n",
            "                    0.00%     382ns         1     382ns     382ns     382ns  cuDeviceGetUuid\n",
            "\n",
            "==540== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "    1920  2.0000MB  2.0000MB  2.0000MB  3.750000GB  335.5646ms  Host To Device\n",
            "      30  4.0000KB  4.0000KB  4.0000KB  120.0000KB  44.64000us  Device To Host\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "nvprof ./cuda_dotproduct1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oE_7eSDp4EN-"
      },
      "source": [
        "##CUDA program WITHOUT cooperative group using grid-stride loop with prefetching+mem advise + sum reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYqOYb1XT8DF"
      },
      "source": [
        "### 1024 threads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGHxqqQsUAE9"
      },
      "source": [
        "#### vector size: 2^20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdt7GOibzCyc",
        "outputId": "f549dae4-0a12-4356-9b8e-a5eb0ba3bd5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting cuda_dotproduct1.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile cuda_dotproduct1.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "\n",
        "__device__\n",
        "double sumBlock(double *dotp_temp2, double val){\n",
        "  int thread_index = threadIdx.x;\n",
        "\n",
        "  for (int i = blockDim.x / 2; i > 0; i /= 2) {\n",
        "    dotp_temp2[thread_index] = val;\n",
        "    __syncthreads();\n",
        "    if(thread_index<i){\n",
        "      val += dotp_temp2[thread_index + i];\n",
        "    }\n",
        "    __syncthreads;\n",
        "  }\n",
        "  return val;\n",
        "}\n",
        "\n",
        "\n",
        "__device__ double atomicAddDouble(double* address, double val){\n",
        "    unsigned long long int* address_as_ull = (unsigned long long int*)address;\n",
        "    unsigned long long int old = *address_as_ull, assumed;\n",
        "    do {\n",
        "        assumed = old;\n",
        "        old = atomicCAS(address_as_ull, assumed, __double_as_longlong(val + __longlong_as_double(assumed)));\n",
        "    } while (assumed != old);\n",
        "    return __longlong_as_double(old);\n",
        "}\n",
        "\n",
        "__global__\n",
        "void sumDot(int n, int numThreads, float* X, float* Y, double* dotp, double* dotp_temp, double* dotp_temp2){\n",
        "\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "  double block_sum = sumBlock(dotp_temp2, dotp_temp[index]);\n",
        "  if (threadIdx.x == 0){\n",
        "    atomicAddDouble(dotp, block_sum);\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__\n",
        "void dotProduct(int n, int numThreads, float* X, float* Y, double* dotp, double* dotp_temp, double* dotp_temp2){\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "\n",
        "  // Fill dotp_temp with the product\n",
        "  for(int i = index; i < n; i += stride){\n",
        "    dotp_temp[i] = X[i] * Y[i];\n",
        "  }\n",
        "}\n",
        "\n",
        "int bulk_logic(unsigned int array_size){\n",
        "  const unsigned int ARRAY_SIZE = array_size;\n",
        "  const unsigned int ARRAY_BYTES = ARRAY_SIZE * sizeof(float);\n",
        "\n",
        "  //declare array\n",
        "  float *X, *Y;\n",
        "  double *dotp, *dotp_temp, *dotp_temp2;\n",
        "  cudaMallocManaged(&X, ARRAY_BYTES);\n",
        "  cudaMallocManaged(&Y, ARRAY_BYTES);\n",
        "  cudaMallocManaged(&dotp, ARRAY_SIZE * sizeof(double));\n",
        "  cudaMallocManaged(&dotp_temp, ARRAY_SIZE * sizeof(double));\n",
        "  cudaMallocManaged(&dotp_temp2, ARRAY_SIZE * sizeof(double));\n",
        "\n",
        "  //prefetch data to create CPU page memory\n",
        "  int device = -1;\n",
        "  cudaGetDevice(&device);\n",
        "\n",
        "  //memory advise\n",
        "  cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId);\n",
        "\n",
        "  //\"prefetch data\" to create CPU page memory\n",
        "  cudaMemPrefetchAsync(X, ARRAY_BYTES, cudaCpuDeviceId, NULL);\n",
        "  cudaMemPrefetchAsync(Y, ARRAY_BYTES, cudaCpuDeviceId, NULL);\n",
        "\n",
        "  //\"prefetch data\" to create GPU page memory\n",
        "  cudaMemPrefetchAsync(dotp, ARRAY_SIZE * sizeof(double), device, NULL);\n",
        "  cudaMemPrefetchAsync(dotp_temp, ARRAY_SIZE * sizeof(double), device, NULL);\n",
        "  cudaMemPrefetchAsync(dotp_temp2, ARRAY_SIZE * sizeof(double), device, NULL);\n",
        "\n",
        "  //initialize array\n",
        "  for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "    X[i] = float(1);\n",
        "    Y[i] = float(1);\n",
        "  }\n",
        "\n",
        "  //(to transfer data from the) Actual transfer: CPU to GPU\n",
        "  cudaMemPrefetchAsync(X, ARRAY_BYTES, device, NULL);\n",
        "  cudaMemPrefetchAsync(Y, ARRAY_BYTES, device, NULL);\n",
        "\n",
        "  //call cuda kernel\n",
        "  int numThreads = 1024;\n",
        "  int numBlocks = (ARRAY_SIZE+numThreads-1)/numThreads;\n",
        "  printf(\"numBlocks = %d, numThreads = %d\\n\", numBlocks, numThreads);\n",
        "\n",
        "  dotProduct<<<numBlocks, numThreads>>> (ARRAY_SIZE, numThreads, X, Y, dotp, dotp_temp, dotp_temp2);\n",
        "  sumDot<<<numBlocks, numThreads>>> (ARRAY_SIZE, numThreads, X, Y, dotp, dotp_temp, dotp_temp2);\n",
        "\n",
        "  //barrier\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  //prefetch\n",
        "  cudaMemPrefetchAsync(dotp, ARRAY_SIZE * sizeof(double), cudaCpuDeviceId, NULL);\n",
        "\n",
        "  //error checking\n",
        "  double expected_dotp  =  0.0;\n",
        "  for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "    expected_dotp = expected_dotp + (X[i] * Y[i]);\n",
        "  }\n",
        "\n",
        "  printf(\"Actual Dot Product: %.2f\\n\", *dotp);\n",
        "  printf(\"Expected Dot Product: %.2f\\n\", expected_dotp);\n",
        "\n",
        "  if(*dotp != expected_dotp){\n",
        "    printf(\"ERROR!: Actual Dot Product and Expected Dot Product is NOT EQUAL\\n\");\n",
        "  }\n",
        "  else{\n",
        "    printf(\"NO ERROR!\\n\");\n",
        "  }\n",
        "\n",
        "  //free memory\n",
        "  cudaFree(X);\n",
        "  cudaFree(Y);\n",
        "  cudaFree(dotp);\n",
        "  cudaFree(dotp_temp);\n",
        "  cudaFree(dotp_temp2);\n",
        "\n",
        "  return 0;\n",
        "}\n",
        "\n",
        "\n",
        "int main(){\n",
        "  int numberOfRuns = 30;\n",
        "  //############CHANGE HERE######################\n",
        "  unsigned int array_size = 1<<20;\n",
        "  //############CHANGE HERE######################\n",
        "\n",
        "  for (int i = 0; i < numberOfRuns; i++) {\n",
        "    bulk_logic(array_size);\n",
        "  }\n",
        "return 0;\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L854SMsyzCyc",
        "outputId": "e261bcc5-ec51-4700-adc3-bf7ad9a9744e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01mcuda_dotproduct1.cu(15)\u001b[0m: \u001b[01;35mwarning\u001b[0m #174-D: expression has no effect\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mcuda_dotproduct1.cu(15)\u001b[0m: \u001b[01;35mwarning\u001b[0m #174-D: expression has no effect\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "nvcc cuda_dotproduct1.cu -o cuda_dotproduct1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWSqMzafzCyc",
        "outputId": "67baec26-b112-49ea-cc55-bf7d47f86a8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==19021== NVPROF is profiling process 19021, command: ./cuda_dotproduct1\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "==19021== Profiling application: ./cuda_dotproduct1\n",
            "==19021== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   84.88%  15.176ms        30  505.87us  500.98us  510.94us  sumDot(int, int, float*, float*, double*, double*, double*)\n",
            "                   15.12%  2.7034ms        30  90.112us  89.471us  90.718us  dotProduct(int, int, float*, float*, double*, double*, double*)\n",
            "      API calls:   58.66%  284.07ms       150  1.8938ms  5.8460us  282.33ms  cudaMallocManaged\n",
            "                   22.80%  110.43ms       240  460.12us  3.6890us  1.3986ms  cudaMemPrefetchAsync\n",
            "                   10.33%  50.037ms        30  1.6679ms  1.5247ms  2.2316ms  cudaDeviceSynchronize\n",
            "                    7.89%  38.232ms       150  254.88us  62.721us  779.74us  cudaFree\n",
            "                    0.22%  1.0568ms        60  17.613us  7.9430us  47.763us  cudaLaunchKernel\n",
            "                    0.05%  244.73us       120  2.0390us  1.0100us  13.183us  cudaMemAdvise\n",
            "                    0.02%  120.60us       101  1.1940us     130ns  50.877us  cuDeviceGetAttribute\n",
            "                    0.01%  34.455us        30  1.1480us     854ns  2.4320us  cudaGetDevice\n",
            "                    0.01%  24.355us         1  24.355us  24.355us  24.355us  cuDeviceGetName\n",
            "                    0.00%  6.4840us         1  6.4840us  6.4840us  6.4840us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.7730us         3     591ns     195ns  1.3450us  cuDeviceGetCount\n",
            "                    0.00%  1.3200us         2     660ns     259ns  1.0610us  cuDeviceGet\n",
            "                    0.00%     485ns         1     485ns     485ns     485ns  cuModuleGetLoadingMode\n",
            "                    0.00%     387ns         1     387ns     387ns     387ns  cuDeviceTotalMem\n",
            "                    0.00%     202ns         1     202ns     202ns     202ns  cuDeviceGetUuid\n",
            "\n",
            "==19021== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "     120  2.0000MB  2.0000MB  2.0000MB  240.0000MB  21.23195ms  Host To Device\n",
            "     120  2.0000MB  2.0000MB  2.0000MB  240.0000MB  19.34811ms  Device To Host\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "nvprof ./cuda_dotproduct1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_VMRcKJU-tW"
      },
      "source": [
        "#### vector size: 2^22"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCfNO5mSU-td",
        "outputId": "91383871-e00c-476f-9c49-69915dce9c7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting cuda_dotproduct1.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile cuda_dotproduct1.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "\n",
        "__device__\n",
        "double sumBlock(double *dotp_temp2, double val){\n",
        "  int thread_index = threadIdx.x;\n",
        "\n",
        "  for (int i = blockDim.x / 2; i > 0; i /= 2) {\n",
        "    dotp_temp2[thread_index] = val;\n",
        "    __syncthreads();\n",
        "    if(thread_index<i){\n",
        "      val += dotp_temp2[thread_index + i];\n",
        "    }\n",
        "    __syncthreads;\n",
        "  }\n",
        "  return val;\n",
        "}\n",
        "\n",
        "\n",
        "__device__ double atomicAddDouble(double* address, double val){\n",
        "    unsigned long long int* address_as_ull = (unsigned long long int*)address;\n",
        "    unsigned long long int old = *address_as_ull, assumed;\n",
        "    do {\n",
        "        assumed = old;\n",
        "        old = atomicCAS(address_as_ull, assumed, __double_as_longlong(val + __longlong_as_double(assumed)));\n",
        "    } while (assumed != old);\n",
        "    return __longlong_as_double(old);\n",
        "}\n",
        "\n",
        "__global__\n",
        "void sumDot(int n, int numThreads, float* X, float* Y, double* dotp, double* dotp_temp, double* dotp_temp2){\n",
        "\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "  double block_sum = sumBlock(dotp_temp2, dotp_temp[index]);\n",
        "  if (threadIdx.x == 0){\n",
        "    atomicAddDouble(dotp, block_sum);\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__\n",
        "void dotProduct(int n, int numThreads, float* X, float* Y, double* dotp, double* dotp_temp, double* dotp_temp2){\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "\n",
        "  // Fill dotp_temp with the product\n",
        "  for(int i = index; i < n; i += stride){\n",
        "    dotp_temp[i] = X[i] * Y[i];\n",
        "  }\n",
        "}\n",
        "\n",
        "int bulk_logic(unsigned int array_size){\n",
        "  const unsigned int ARRAY_SIZE = array_size;\n",
        "  const unsigned int ARRAY_BYTES = ARRAY_SIZE * sizeof(float);\n",
        "\n",
        "  //declare array\n",
        "  float *X, *Y;\n",
        "  double *dotp, *dotp_temp, *dotp_temp2;\n",
        "  cudaMallocManaged(&X, ARRAY_BYTES);\n",
        "  cudaMallocManaged(&Y, ARRAY_BYTES);\n",
        "  cudaMallocManaged(&dotp, ARRAY_SIZE * sizeof(double));\n",
        "  cudaMallocManaged(&dotp_temp, ARRAY_SIZE * sizeof(double));\n",
        "  cudaMallocManaged(&dotp_temp2, ARRAY_SIZE * sizeof(double));\n",
        "\n",
        "  //prefetch data to create CPU page memory\n",
        "  int device = -1;\n",
        "  cudaGetDevice(&device);\n",
        "\n",
        "  //memory advise\n",
        "  cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId);\n",
        "\n",
        "  //\"prefetch data\" to create CPU page memory\n",
        "  cudaMemPrefetchAsync(X, ARRAY_BYTES, cudaCpuDeviceId, NULL);\n",
        "  cudaMemPrefetchAsync(Y, ARRAY_BYTES, cudaCpuDeviceId, NULL);\n",
        "\n",
        "  //\"prefetch data\" to create GPU page memory\n",
        "  cudaMemPrefetchAsync(dotp, ARRAY_SIZE * sizeof(double), device, NULL);\n",
        "  cudaMemPrefetchAsync(dotp_temp, ARRAY_SIZE * sizeof(double), device, NULL);\n",
        "  cudaMemPrefetchAsync(dotp_temp2, ARRAY_SIZE * sizeof(double), device, NULL);\n",
        "\n",
        "  //initialize array\n",
        "  for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "    X[i] = float(1);\n",
        "    Y[i] = float(1);\n",
        "  }\n",
        "\n",
        "  //(to transfer data from the) Actual transfer: CPU to GPU\n",
        "  cudaMemPrefetchAsync(X, ARRAY_BYTES, device, NULL);\n",
        "  cudaMemPrefetchAsync(Y, ARRAY_BYTES, device, NULL);\n",
        "\n",
        "  //call cuda kernel\n",
        "  int numThreads = 1024;\n",
        "  int numBlocks = (ARRAY_SIZE+numThreads-1)/numThreads;\n",
        "  printf(\"numBlocks = %d, numThreads = %d\\n\", numBlocks, numThreads);\n",
        "\n",
        "  dotProduct<<<numBlocks, numThreads>>> (ARRAY_SIZE, numThreads, X, Y, dotp, dotp_temp, dotp_temp2);\n",
        "  sumDot<<<numBlocks, numThreads>>> (ARRAY_SIZE, numThreads, X, Y, dotp, dotp_temp, dotp_temp2);\n",
        "\n",
        "  //barrier\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  //prefetch\n",
        "  cudaMemPrefetchAsync(dotp, ARRAY_SIZE * sizeof(double), cudaCpuDeviceId, NULL);\n",
        "\n",
        "  //error checking\n",
        "  double expected_dotp  =  0.0;\n",
        "  for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "    expected_dotp = expected_dotp + (X[i] * Y[i]);\n",
        "  }\n",
        "\n",
        "  printf(\"Actual Dot Product: %.2f\\n\", *dotp);\n",
        "  printf(\"Expected Dot Product: %.2f\\n\", expected_dotp);\n",
        "\n",
        "  if(*dotp != expected_dotp){\n",
        "    printf(\"ERROR!: Actual Dot Product and Expected Dot Product is NOT EQUAL\\n\");\n",
        "  }\n",
        "  else{\n",
        "    printf(\"NO ERROR!\\n\");\n",
        "  }\n",
        "\n",
        "  //free memory\n",
        "  cudaFree(X);\n",
        "  cudaFree(Y);\n",
        "  cudaFree(dotp);\n",
        "  cudaFree(dotp_temp);\n",
        "  cudaFree(dotp_temp2);\n",
        "\n",
        "  return 0;\n",
        "}\n",
        "\n",
        "\n",
        "int main(){\n",
        "  int numberOfRuns = 30;\n",
        "  //############CHANGE HERE######################\n",
        "  unsigned int array_size = 1<<22;\n",
        "  //############CHANGE HERE######################\n",
        "\n",
        "  for (int i = 0; i < numberOfRuns; i++) {\n",
        "    bulk_logic(array_size);\n",
        "  }\n",
        "return 0;\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOaV_ljJU-td",
        "outputId": "c529b5b3-1084-4ca6-fdb7-3f31033ca4fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01mcuda_dotproduct1.cu(15)\u001b[0m: \u001b[01;35mwarning\u001b[0m #174-D: expression has no effect\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mcuda_dotproduct1.cu(15)\u001b[0m: \u001b[01;35mwarning\u001b[0m #174-D: expression has no effect\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "nvcc cuda_dotproduct1.cu -o cuda_dotproduct1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKKTjuNUU-td",
        "outputId": "2d92029e-0806-43f2-a115-e69d3baed5e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==19336== NVPROF is profiling process 19336, command: ./cuda_dotproduct1\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "==19336== Profiling application: ./cuda_dotproduct1\n",
            "==19336== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   84.96%  58.712ms        30  1.9571ms  1.9557ms  1.9582ms  sumDot(int, int, float*, float*, double*, double*, double*)\n",
            "                   15.04%  10.394ms        30  346.47us  345.63us  347.61us  dotProduct(int, int, float*, float*, double*, double*, double*)\n",
            "      API calls:   54.25%  721.74ms       240  3.0073ms  4.5280us  12.672ms  cudaMemPrefetchAsync\n",
            "                   21.35%  284.02ms       150  1.8935ms  238.96us  7.0001ms  cudaFree\n",
            "                   18.64%  248.01ms       150  1.6534ms  8.7040us  244.44ms  cudaMallocManaged\n",
            "                    5.57%  74.044ms        30  2.4681ms  2.2829ms  4.9154ms  cudaDeviceSynchronize\n",
            "                    0.15%  2.0018ms        60  33.363us  10.290us  75.580us  cudaLaunchKernel\n",
            "                    0.03%  373.22us       120  3.1100us  1.0110us  10.744us  cudaMemAdvise\n",
            "                    0.01%  124.55us       101  1.2330us     137ns  52.236us  cuDeviceGetAttribute\n",
            "                    0.01%  89.094us        30  2.9690us  2.0210us  3.9520us  cudaGetDevice\n",
            "                    0.00%  24.650us         1  24.650us  24.650us  24.650us  cuDeviceGetName\n",
            "                    0.00%  9.0750us         1  9.0750us  9.0750us  9.0750us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.9880us         3     662ns     228ns  1.4430us  cuDeviceGetCount\n",
            "                    0.00%     991ns         2     495ns     291ns     700ns  cuDeviceGet\n",
            "                    0.00%     544ns         1     544ns     544ns     544ns  cuModuleGetLoadingMode\n",
            "                    0.00%     358ns         1     358ns     358ns     358ns  cuDeviceTotalMem\n",
            "                    0.00%     234ns         1     234ns     234ns     234ns  cuDeviceGetUuid\n",
            "\n",
            "==19336== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "     480  2.0000MB  2.0000MB  2.0000MB  960.0000MB  85.05751ms  Host To Device\n",
            "     480  2.0000MB  2.0000MB  2.0000MB  960.0000MB  77.36938ms  Device To Host\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "nvprof ./cuda_dotproduct1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtUVkfREU_zG"
      },
      "source": [
        "#### vector size: 2^24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0UyWVy-U_zG",
        "outputId": "ade18b2a-1343-45fd-d840-c815f43ba1a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting cuda_dotproduct1.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile cuda_dotproduct1.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "\n",
        "__device__\n",
        "double sumBlock(double *dotp_temp2, double val){\n",
        "  int thread_index = threadIdx.x;\n",
        "\n",
        "  for (int i = blockDim.x / 2; i > 0; i /= 2) {\n",
        "    dotp_temp2[thread_index] = val;\n",
        "    __syncthreads();\n",
        "    if(thread_index<i){\n",
        "      val += dotp_temp2[thread_index + i];\n",
        "    }\n",
        "    __syncthreads;\n",
        "  }\n",
        "  return val;\n",
        "}\n",
        "\n",
        "\n",
        "__device__ double atomicAddDouble(double* address, double val){\n",
        "    unsigned long long int* address_as_ull = (unsigned long long int*)address;\n",
        "    unsigned long long int old = *address_as_ull, assumed;\n",
        "    do {\n",
        "        assumed = old;\n",
        "        old = atomicCAS(address_as_ull, assumed, __double_as_longlong(val + __longlong_as_double(assumed)));\n",
        "    } while (assumed != old);\n",
        "    return __longlong_as_double(old);\n",
        "}\n",
        "\n",
        "__global__\n",
        "void sumDot(int n, int numThreads, float* X, float* Y, double* dotp, double* dotp_temp, double* dotp_temp2){\n",
        "\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "  double block_sum = sumBlock(dotp_temp2, dotp_temp[index]);\n",
        "  if (threadIdx.x == 0){\n",
        "    atomicAddDouble(dotp, block_sum);\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__\n",
        "void dotProduct(int n, int numThreads, float* X, float* Y, double* dotp, double* dotp_temp, double* dotp_temp2){\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "\n",
        "  // Fill dotp_temp with the product\n",
        "  for(int i = index; i < n; i += stride){\n",
        "    dotp_temp[i] = X[i] * Y[i];\n",
        "  }\n",
        "}\n",
        "\n",
        "int bulk_logic(unsigned int array_size){\n",
        "  const unsigned int ARRAY_SIZE = array_size;\n",
        "  const unsigned int ARRAY_BYTES = ARRAY_SIZE * sizeof(float);\n",
        "\n",
        "  //declare array\n",
        "  float *X, *Y;\n",
        "  double *dotp, *dotp_temp, *dotp_temp2;\n",
        "  cudaMallocManaged(&X, ARRAY_BYTES);\n",
        "  cudaMallocManaged(&Y, ARRAY_BYTES);\n",
        "  cudaMallocManaged(&dotp, ARRAY_SIZE * sizeof(double));\n",
        "  cudaMallocManaged(&dotp_temp, ARRAY_SIZE * sizeof(double));\n",
        "  cudaMallocManaged(&dotp_temp2, ARRAY_SIZE * sizeof(double));\n",
        "\n",
        "  //prefetch data to create CPU page memory\n",
        "  int device = -1;\n",
        "  cudaGetDevice(&device);\n",
        "\n",
        "  //memory advise\n",
        "  cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId);\n",
        "\n",
        "  //\"prefetch data\" to create CPU page memory\n",
        "  cudaMemPrefetchAsync(X, ARRAY_BYTES, cudaCpuDeviceId, NULL);\n",
        "  cudaMemPrefetchAsync(Y, ARRAY_BYTES, cudaCpuDeviceId, NULL);\n",
        "\n",
        "  //\"prefetch data\" to create GPU page memory\n",
        "  cudaMemPrefetchAsync(dotp, ARRAY_SIZE * sizeof(double), device, NULL);\n",
        "  cudaMemPrefetchAsync(dotp_temp, ARRAY_SIZE * sizeof(double), device, NULL);\n",
        "  cudaMemPrefetchAsync(dotp_temp2, ARRAY_SIZE * sizeof(double), device, NULL);\n",
        "\n",
        "  //initialize array\n",
        "  for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "    X[i] = float(1);\n",
        "    Y[i] = float(1);\n",
        "  }\n",
        "\n",
        "  //(to transfer data from the) Actual transfer: CPU to GPU\n",
        "  cudaMemPrefetchAsync(X, ARRAY_BYTES, device, NULL);\n",
        "  cudaMemPrefetchAsync(Y, ARRAY_BYTES, device, NULL);\n",
        "\n",
        "  //call cuda kernel\n",
        "  int numThreads = 1024;\n",
        "  int numBlocks = (ARRAY_SIZE+numThreads-1)/numThreads;\n",
        "  printf(\"numBlocks = %d, numThreads = %d\\n\", numBlocks, numThreads);\n",
        "\n",
        "  dotProduct<<<numBlocks, numThreads>>> (ARRAY_SIZE, numThreads, X, Y, dotp, dotp_temp, dotp_temp2);\n",
        "  sumDot<<<numBlocks, numThreads>>> (ARRAY_SIZE, numThreads, X, Y, dotp, dotp_temp, dotp_temp2);\n",
        "\n",
        "  //barrier\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  //prefetch\n",
        "  cudaMemPrefetchAsync(dotp, ARRAY_SIZE * sizeof(double), cudaCpuDeviceId, NULL);\n",
        "\n",
        "  //error checking\n",
        "  double expected_dotp  =  0.0;\n",
        "  for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "    expected_dotp = expected_dotp + (X[i] * Y[i]);\n",
        "  }\n",
        "\n",
        "  printf(\"Actual Dot Product: %.2f\\n\", *dotp);\n",
        "  printf(\"Expected Dot Product: %.2f\\n\", expected_dotp);\n",
        "\n",
        "  if(*dotp != expected_dotp){\n",
        "    printf(\"ERROR!: Actual Dot Product and Expected Dot Product is NOT EQUAL\\n\");\n",
        "  }\n",
        "  else{\n",
        "    printf(\"NO ERROR!\\n\");\n",
        "  }\n",
        "\n",
        "  //free memory\n",
        "  cudaFree(X);\n",
        "  cudaFree(Y);\n",
        "  cudaFree(dotp);\n",
        "  cudaFree(dotp_temp);\n",
        "  cudaFree(dotp_temp2);\n",
        "\n",
        "  return 0;\n",
        "}\n",
        "\n",
        "\n",
        "int main(){\n",
        "  int numberOfRuns = 30;\n",
        "  //############CHANGE HERE######################\n",
        "  unsigned int array_size = 1<<24;\n",
        "  //############CHANGE HERE######################\n",
        "\n",
        "  for (int i = 0; i < numberOfRuns; i++) {\n",
        "    bulk_logic(array_size);\n",
        "  }\n",
        "return 0;\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lpfcF8iU_zH",
        "outputId": "16a4de2b-0cd8-4154-fe26-e672fad57cc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01mcuda_dotproduct1.cu(15)\u001b[0m: \u001b[01;35mwarning\u001b[0m #174-D: expression has no effect\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mcuda_dotproduct1.cu(15)\u001b[0m: \u001b[01;35mwarning\u001b[0m #174-D: expression has no effect\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "nvcc cuda_dotproduct1.cu -o cuda_dotproduct1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-z-uaqfU_zH",
        "outputId": "37d49a37-c365-4875-f37d-1117d05ef25a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==19749== NVPROF is profiling process 19749, command: ./cuda_dotproduct1\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "==19749== Profiling application: ./cuda_dotproduct1\n",
            "==19749== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   85.08%  234.33ms        30  7.8109ms  7.8090ms  7.8129ms  sumDot(int, int, float*, float*, double*, double*, double*)\n",
            "                   14.92%  41.092ms        30  1.3697ms  1.3685ms  1.3715ms  dotProduct(int, int, float*, float*, double*, double*, double*)\n",
            "      API calls:   63.08%  2.04988s       240  8.5412ms  3.8550us  43.002ms  cudaMemPrefetchAsync\n",
            "                   20.70%  672.56ms       150  4.4837ms  902.07us  12.134ms  cudaFree\n",
            "                    8.47%  275.34ms        30  9.1781ms  9.1728ms  9.1848ms  cudaDeviceSynchronize\n",
            "                    7.68%  249.71ms       150  1.6648ms  10.672us  246.59ms  cudaMallocManaged\n",
            "                    0.05%  1.7536ms        60  29.227us  7.9100us  55.293us  cudaLaunchKernel\n",
            "                    0.01%  310.07us       120  2.5830us  1.0920us  12.709us  cudaMemAdvise\n",
            "                    0.00%  132.10us       101  1.3070us     129ns  54.506us  cuDeviceGetAttribute\n",
            "                    0.00%  80.535us        30  2.6840us  2.2480us  3.0390us  cudaGetDevice\n",
            "                    0.00%  29.232us         1  29.232us  29.232us  29.232us  cuDeviceGetName\n",
            "                    0.00%  9.1440us         1  9.1440us  9.1440us  9.1440us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.0290us         3     676ns     217ns  1.5710us  cuDeviceGetCount\n",
            "                    0.00%     976ns         2     488ns     148ns     828ns  cuDeviceGet\n",
            "                    0.00%     571ns         1     571ns     571ns     571ns  cuDeviceTotalMem\n",
            "                    0.00%     302ns         1     302ns     302ns     302ns  cuModuleGetLoadingMode\n",
            "                    0.00%     249ns         1     249ns     249ns     249ns  cuDeviceGetUuid\n",
            "\n",
            "==19749== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "    1920  2.0000MB  2.0000MB  2.0000MB  3.750000GB  339.3905ms  Host To Device\n",
            "    1920  2.0000MB  2.0000MB  2.0000MB  3.750000GB  309.4843ms  Device To Host\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "nvprof ./cuda_dotproduct1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Yd_BH4PAOM3"
      },
      "source": [
        "##CUDA program using cooperative using grid-stride loop with prefetching+mem advise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hecCYWnIgFp1"
      },
      "source": [
        "###1024 threads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkMPxcQ5gqMc"
      },
      "source": [
        "####vector size: 2^20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOj-8Q8-g08E",
        "outputId": "c7f0bf34-914c-49b1-ca8d-e39a25e669f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing cuda_dotproduct2.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile cuda_dotproduct2.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cooperative_groups.h>\n",
        "\n",
        "using namespace cooperative_groups;\n",
        "\n",
        "__device__ double atomicAddDouble(double* address, double val){\n",
        "    unsigned long long int* address_as_ull = (unsigned long long int*)address;\n",
        "    unsigned long long int old = *address_as_ull, assumed;\n",
        "    do {\n",
        "        assumed = old;\n",
        "        old = atomicCAS(address_as_ull, assumed, __double_as_longlong(val + __longlong_as_double(assumed)));\n",
        "    } while (assumed != old);\n",
        "    return __longlong_as_double(old);\n",
        "}\n",
        "\n",
        "__device__\n",
        "double sumBlock(thread_group g, double *dotp_temp2, double val){\n",
        "  int thread_index = g.thread_rank();\n",
        "\n",
        "  for (int i = blockDim.x / 2; i > 0; i /= 2) {\n",
        "    dotp_temp2[thread_index] = val;\n",
        "    g.sync();\n",
        "    if(thread_index<i){\n",
        "      val += dotp_temp2[thread_index + i];\n",
        "    }\n",
        "    g.sync();\n",
        "  }\n",
        "\n",
        "  return val;\n",
        "}\n",
        "\n",
        "__global__\n",
        "void sumDot(int n, int numThreads, float* X, float* Y, double* dotp, double* dotp_temp, double* dotp_temp2){\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "  thread_block g = this_thread_block();\n",
        "  g.sync();\n",
        "  double block_sum = sumBlock(g, dotp_temp2, dotp_temp[index]);\n",
        "  g.sync();\n",
        "\n",
        "  if (threadIdx.x == 0){\n",
        "    atomicAddDouble(dotp, block_sum);\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__\n",
        "void dotProduct(int n, int numThreads, float* X, float* Y, double* dotp, double* dotp_temp, double* dotp_temp2){\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "\n",
        "  // Fill dotp_temp with the product\n",
        "  for(int i = index; i < n; i += stride){\n",
        "    dotp_temp[i] = X[i] * Y[i];\n",
        "  }\n",
        "}\n",
        "\n",
        "int bulk_logic(unsigned int array_size){\n",
        "  const unsigned int ARRAY_SIZE = array_size;\n",
        "  const unsigned int ARRAY_BYTES = ARRAY_SIZE * sizeof(float);\n",
        "\n",
        "  //declare array\n",
        "  float *X, *Y;\n",
        "  double *dotp, *dotp_temp, *dotp_temp2;\n",
        "  cudaMallocManaged(&X, ARRAY_BYTES);\n",
        "  cudaMallocManaged(&Y, ARRAY_BYTES);\n",
        "  cudaMallocManaged(&dotp, ARRAY_SIZE * sizeof(double));\n",
        "  cudaMallocManaged(&dotp_temp, ARRAY_SIZE * sizeof(double));\n",
        "  cudaMallocManaged(&dotp_temp2, ARRAY_SIZE * sizeof(double));\n",
        "\n",
        "  //prefetch data to create CPU page memory\n",
        "  int device = -1;\n",
        "  cudaGetDevice(&device);\n",
        "\n",
        "  //memory advise\n",
        "  cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId);\n",
        "\n",
        "  //\"prefetch data\" to create CPU page memory\n",
        "  cudaMemPrefetchAsync(X, ARRAY_BYTES, cudaCpuDeviceId, NULL);\n",
        "  cudaMemPrefetchAsync(Y, ARRAY_BYTES, cudaCpuDeviceId, NULL);\n",
        "\n",
        "  //\"prefetch data\" to create GPU page memory\n",
        "  cudaMemPrefetchAsync(dotp, ARRAY_SIZE * sizeof(double), device, NULL);\n",
        "  cudaMemPrefetchAsync(dotp_temp, ARRAY_SIZE * sizeof(double), device, NULL);\n",
        "  cudaMemPrefetchAsync(dotp_temp2, ARRAY_SIZE * sizeof(double), device, NULL);\n",
        "\n",
        "  //initialize array\n",
        "  for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "    X[i] = float(1);\n",
        "    Y[i] = float(1);\n",
        "  }\n",
        "\n",
        "  //(to transfer data from the) Actual transfer: CPU to GPU\n",
        "  cudaMemPrefetchAsync(X, ARRAY_BYTES, device, NULL);\n",
        "  cudaMemPrefetchAsync(Y, ARRAY_BYTES, device, NULL);\n",
        "\n",
        "  //call cuda kernel\n",
        "  int numThreads = 1024;\n",
        "  int numBlocks = (ARRAY_SIZE+numThreads-1)/numThreads;\n",
        "  printf(\"numBlocks = %d, numThreads = %d\\n\", numBlocks, numThreads);\n",
        "\n",
        "  dotProduct<<<numBlocks, numThreads>>> (ARRAY_SIZE, numThreads, X, Y, dotp, dotp_temp, dotp_temp2);\n",
        "  sumDot<<<numBlocks, numThreads>>> (ARRAY_SIZE, numThreads, X, Y, dotp, dotp_temp, dotp_temp2);\n",
        "\n",
        "  //barrier\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  //memory advise\n",
        "  cudaMemAdvise(dotp, ARRAY_SIZE * sizeof(double), cudaMemAdviseSetReadMostly, device);\n",
        "  cudaMemPrefetchAsync(dotp, ARRAY_SIZE * sizeof(double), cudaCpuDeviceId, NULL);\n",
        "\n",
        "  //error checking\n",
        "  double expected_dotp  =  0.0;\n",
        "  for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "    expected_dotp = expected_dotp + (X[i] * Y[i]);\n",
        "  }\n",
        "\n",
        "  printf(\"Actual Dot Product: %.2f\\n\", *dotp);\n",
        "  printf(\"Expected Dot Product: %.2f\\n\", expected_dotp);\n",
        "\n",
        "  if(*dotp != expected_dotp){\n",
        "    printf(\"ERROR!: Actual Dot Product and Expected Dot Product is NOT EQUAL\\n\");\n",
        "  }\n",
        "  else{\n",
        "    printf(\"NO ERROR!\\n\");\n",
        "  }\n",
        "\n",
        "  //free memory\n",
        "  cudaFree(X);\n",
        "  cudaFree(Y);\n",
        "  cudaFree(dotp);\n",
        "  cudaFree(dotp_temp);\n",
        "  cudaFree(dotp_temp2);\n",
        "\n",
        "  return 0;\n",
        "}\n",
        "\n",
        "int main(){\n",
        "  int numberOfRuns = 30;\n",
        "  //############CHANGE HERE######################\n",
        "  unsigned int array_size = 1<<20;\n",
        "  //############CHANGE HERE######################\n",
        "\n",
        "  for (int i = 0; i < numberOfRuns; i++) {\n",
        "    bulk_logic(array_size);\n",
        "  }\n",
        "return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNPoCZPoAOM4",
        "outputId": "bf31c25f-9b74-4fe4-bc31-4d31ea1140cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "nvcc cuda_dotproduct2.cu -o cuda_dotproduct2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYTFgAtQAOM4",
        "outputId": "e675b9e2-e7ac-4d75-b88a-0bc42329ce45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==23639== NVPROF is profiling process 23639, command: ./cuda_dotproduct2\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "numBlocks = 1024, numThreads = 1024\n",
            "Actual Dot Product: 1048576.00\n",
            "Expected Dot Product: 1048576.00\n",
            "NO ERROR!\n",
            "==23639== Profiling application: ./cuda_dotproduct2\n",
            "==23639== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   85.25%  15.616ms        30  520.52us  517.82us  525.08us  sumDot(int, int, float*, float*, double*, double*, double*)\n",
            "                   14.75%  2.7016ms        30  90.054us  89.567us  90.846us  dotProduct(int, int, float*, float*, double*, double*, double*)\n",
            "      API calls:   58.42%  281.23ms       150  1.8748ms  6.0610us  279.62ms  cudaMallocManaged\n",
            "                   22.89%  110.19ms       240  459.13us  3.7380us  1.5419ms  cudaMemPrefetchAsync\n",
            "                   10.50%  50.538ms        30  1.6846ms  1.5293ms  2.9499ms  cudaDeviceSynchronize\n",
            "                    7.78%  37.434ms       150  249.56us  63.016us  745.14us  cudaFree\n",
            "                    0.21%  1.0298ms        60  17.162us  7.6720us  46.057us  cudaLaunchKernel\n",
            "                    0.16%  747.36us       150  4.9820us     991ns  26.500us  cudaMemAdvise\n",
            "                    0.03%  122.94us       101  1.2170us     130ns  51.606us  cuDeviceGetAttribute\n",
            "                    0.01%  37.235us        30  1.2410us  1.0110us  2.2890us  cudaGetDevice\n",
            "                    0.00%  23.310us         1  23.310us  23.310us  23.310us  cuDeviceGetName\n",
            "                    0.00%  6.7480us         1  6.7480us  6.7480us  6.7480us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.3660us         3     788ns     225ns  1.9030us  cuDeviceGetCount\n",
            "                    0.00%  1.0080us         2     504ns     190ns     818ns  cuDeviceGet\n",
            "                    0.00%     527ns         1     527ns     527ns     527ns  cuModuleGetLoadingMode\n",
            "                    0.00%     375ns         1     375ns     375ns     375ns  cuDeviceTotalMem\n",
            "                    0.00%     228ns         1     228ns     228ns     228ns  cuDeviceGetUuid\n",
            "\n",
            "==23639== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "     120  2.0000MB  2.0000MB  2.0000MB  240.0000MB  21.29472ms  Host To Device\n",
            "     120  2.0000MB  2.0000MB  2.0000MB  240.0000MB  19.34820ms  Device To Host\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "nvprof ./cuda_dotproduct2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKla3j_yiP33"
      },
      "source": [
        "####vector size: 2^22"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iviJEv6riSpK",
        "outputId": "6daee317-04b6-42ac-86ec-712b548a4dfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting cuda_dotproduct2.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile cuda_dotproduct2.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cooperative_groups.h>\n",
        "\n",
        "using namespace cooperative_groups;\n",
        "\n",
        "__device__ double atomicAddDouble(double* address, double val){\n",
        "    unsigned long long int* address_as_ull = (unsigned long long int*)address;\n",
        "    unsigned long long int old = *address_as_ull, assumed;\n",
        "    do {\n",
        "        assumed = old;\n",
        "        old = atomicCAS(address_as_ull, assumed, __double_as_longlong(val + __longlong_as_double(assumed)));\n",
        "    } while (assumed != old);\n",
        "    return __longlong_as_double(old);\n",
        "}\n",
        "\n",
        "__device__\n",
        "double sumBlock(thread_group g, double *dotp_temp2, double val){\n",
        "  int thread_index = g.thread_rank();\n",
        "\n",
        "  for (int i = blockDim.x / 2; i > 0; i /= 2) {\n",
        "    dotp_temp2[thread_index] = val;\n",
        "    g.sync();\n",
        "    if(thread_index<i){\n",
        "      val += dotp_temp2[thread_index + i];\n",
        "    }\n",
        "    g.sync();\n",
        "  }\n",
        "\n",
        "  return val;\n",
        "}\n",
        "\n",
        "__global__\n",
        "void sumDot(int n, int numThreads, float* X, float* Y, double* dotp, double* dotp_temp, double* dotp_temp2){\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "  thread_block g = this_thread_block();\n",
        "  g.sync();\n",
        "  double block_sum = sumBlock(g, dotp_temp2, dotp_temp[index]);\n",
        "  g.sync();\n",
        "\n",
        "  if (threadIdx.x == 0){\n",
        "    atomicAddDouble(dotp, block_sum);\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__\n",
        "void dotProduct(int n, int numThreads, float* X, float* Y, double* dotp, double* dotp_temp, double* dotp_temp2){\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "\n",
        "  // Fill dotp_temp with the product\n",
        "  for(int i = index; i < n; i += stride){\n",
        "    dotp_temp[i] = X[i] * Y[i];\n",
        "  }\n",
        "}\n",
        "\n",
        "int bulk_logic(unsigned int array_size){\n",
        "  const unsigned int ARRAY_SIZE = array_size;\n",
        "  const unsigned int ARRAY_BYTES = ARRAY_SIZE * sizeof(float);\n",
        "\n",
        "  //declare array\n",
        "  float *X, *Y;\n",
        "  double *dotp, *dotp_temp, *dotp_temp2;\n",
        "  cudaMallocManaged(&X, ARRAY_BYTES);\n",
        "  cudaMallocManaged(&Y, ARRAY_BYTES);\n",
        "  cudaMallocManaged(&dotp, ARRAY_SIZE * sizeof(double));\n",
        "  cudaMallocManaged(&dotp_temp, ARRAY_SIZE * sizeof(double));\n",
        "  cudaMallocManaged(&dotp_temp2, ARRAY_SIZE * sizeof(double));\n",
        "\n",
        "  //prefetch data to create CPU page memory\n",
        "  int device = -1;\n",
        "  cudaGetDevice(&device);\n",
        "\n",
        "  //memory advise\n",
        "  cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId);\n",
        "\n",
        "  //\"prefetch data\" to create CPU page memory\n",
        "  cudaMemPrefetchAsync(X, ARRAY_BYTES, cudaCpuDeviceId, NULL);\n",
        "  cudaMemPrefetchAsync(Y, ARRAY_BYTES, cudaCpuDeviceId, NULL);\n",
        "\n",
        "  //\"prefetch data\" to create GPU page memory\n",
        "  cudaMemPrefetchAsync(dotp, ARRAY_SIZE * sizeof(double), device, NULL);\n",
        "  cudaMemPrefetchAsync(dotp_temp, ARRAY_SIZE * sizeof(double), device, NULL);\n",
        "  cudaMemPrefetchAsync(dotp_temp2, ARRAY_SIZE * sizeof(double), device, NULL);\n",
        "\n",
        "  //initialize array\n",
        "  for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "    X[i] = float(1);\n",
        "    Y[i] = float(1);\n",
        "  }\n",
        "\n",
        "  //(to transfer data from the) Actual transfer: CPU to GPU\n",
        "  cudaMemPrefetchAsync(X, ARRAY_BYTES, device, NULL);\n",
        "  cudaMemPrefetchAsync(Y, ARRAY_BYTES, device, NULL);\n",
        "\n",
        "  //call cuda kernel\n",
        "  int numThreads = 1024;\n",
        "  int numBlocks = (ARRAY_SIZE+numThreads-1)/numThreads;\n",
        "  printf(\"numBlocks = %d, numThreads = %d\\n\", numBlocks, numThreads);\n",
        "\n",
        "  dotProduct<<<numBlocks, numThreads>>> (ARRAY_SIZE, numThreads, X, Y, dotp, dotp_temp, dotp_temp2);\n",
        "  sumDot<<<numBlocks, numThreads>>> (ARRAY_SIZE, numThreads, X, Y, dotp, dotp_temp, dotp_temp2);\n",
        "\n",
        "  //barrier\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  //memory advise\n",
        "  cudaMemAdvise(dotp, ARRAY_SIZE * sizeof(double), cudaMemAdviseSetReadMostly, device);\n",
        "  cudaMemPrefetchAsync(dotp, ARRAY_SIZE * sizeof(double), cudaCpuDeviceId, NULL);\n",
        "\n",
        "  //error checking\n",
        "  double expected_dotp  =  0.0;\n",
        "  for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "    expected_dotp = expected_dotp + (X[i] * Y[i]);\n",
        "  }\n",
        "\n",
        "  printf(\"Actual Dot Product: %.2f\\n\", *dotp);\n",
        "  printf(\"Expected Dot Product: %.2f\\n\", expected_dotp);\n",
        "\n",
        "  if(*dotp != expected_dotp){\n",
        "    printf(\"ERROR!: Actual Dot Product and Expected Dot Product is NOT EQUAL\\n\");\n",
        "  }\n",
        "  else{\n",
        "    printf(\"NO ERROR!\\n\");\n",
        "  }\n",
        "\n",
        "  //free memory\n",
        "  cudaFree(X);\n",
        "  cudaFree(Y);\n",
        "  cudaFree(dotp);\n",
        "  cudaFree(dotp_temp);\n",
        "  cudaFree(dotp_temp2);\n",
        "\n",
        "  return 0;\n",
        "}\n",
        "\n",
        "int main(){\n",
        "  int numberOfRuns = 30;\n",
        "  //############CHANGE HERE######################\n",
        "  unsigned int array_size = 1<<22;\n",
        "  //############CHANGE HERE######################\n",
        "\n",
        "  for (int i = 0; i < numberOfRuns; i++) {\n",
        "    bulk_logic(array_size);\n",
        "  }\n",
        "return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7y2CpPw8iUk7",
        "outputId": "0b9798d2-6a24-49c4-f835-0e060e1328b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "nvcc cuda_dotproduct2.cu -o cuda_dotproduct2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1BqY6F7iYfz",
        "outputId": "5e79633e-e5af-40df-a98d-0bfab1b2e411"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==24278== NVPROF is profiling process 24278, command: ./cuda_dotproduct2\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "numBlocks = 4096, numThreads = 1024\n",
            "Actual Dot Product: 4194304.00\n",
            "Expected Dot Product: 4194304.00\n",
            "NO ERROR!\n",
            "==24278== Profiling application: ./cuda_dotproduct2\n",
            "==24278== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   85.32%  60.420ms        30  2.0140ms  2.0121ms  2.0153ms  sumDot(int, int, float*, float*, double*, double*, double*)\n",
            "                   14.68%  10.392ms        30  346.40us  345.59us  347.32us  dotProduct(int, int, float*, float*, double*, double*, double*)\n",
            "      API calls:   44.17%  455.80ms       240  1.8992ms  3.9410us  7.5213ms  cudaMemPrefetchAsync\n",
            "                   23.63%  243.85ms       150  1.6256ms  8.1180us  241.04ms  cudaMallocManaged\n",
            "                   17.90%  184.69ms       150  1.2313ms  230.94us  3.9077ms  cudaFree\n",
            "                   13.90%  143.46ms        30  4.7819ms  2.3556ms  6.2872ms  cudaDeviceSynchronize\n",
            "                    0.21%  2.2144ms        60  36.906us  7.1780us  181.12us  cudaLaunchKernel\n",
            "                    0.16%  1.6981ms       150  11.320us  1.0260us  87.293us  cudaMemAdvise\n",
            "                    0.01%  127.25us       101  1.2590us     134ns  53.679us  cuDeviceGetAttribute\n",
            "                    0.01%  78.594us        30  2.6190us  2.1550us  7.2680us  cudaGetDevice\n",
            "                    0.00%  25.223us         1  25.223us  25.223us  25.223us  cuDeviceGetName\n",
            "                    0.00%  6.7020us         1  6.7020us  6.7020us  6.7020us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.4540us         3     818ns     233ns  1.8460us  cuDeviceGetCount\n",
            "                    0.00%     940ns         2     470ns     168ns     772ns  cuDeviceGet\n",
            "                    0.00%     490ns         1     490ns     490ns     490ns  cuModuleGetLoadingMode\n",
            "                    0.00%     385ns         1     385ns     385ns     385ns  cuDeviceTotalMem\n",
            "                    0.00%     221ns         1     221ns     221ns     221ns  cuDeviceGetUuid\n",
            "\n",
            "==24278== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "     480  2.0000MB  2.0000MB  2.0000MB  960.0000MB  85.75930ms  Host To Device\n",
            "     480  2.0000MB  2.0000MB  2.0000MB  960.0000MB  77.32606ms  Device To Host\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "nvprof ./cuda_dotproduct2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsHfiXpci1yx"
      },
      "source": [
        "####vector size: 2^24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdVUih2yi4iS",
        "outputId": "6d6ed57b-d2bc-4168-e1f9-66661fe73e1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting cuda_dotproduct2.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile cuda_dotproduct2.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cooperative_groups.h>\n",
        "\n",
        "using namespace cooperative_groups;\n",
        "\n",
        "__device__ double atomicAddDouble(double* address, double val){\n",
        "    unsigned long long int* address_as_ull = (unsigned long long int*)address;\n",
        "    unsigned long long int old = *address_as_ull, assumed;\n",
        "    do {\n",
        "        assumed = old;\n",
        "        old = atomicCAS(address_as_ull, assumed, __double_as_longlong(val + __longlong_as_double(assumed)));\n",
        "    } while (assumed != old);\n",
        "    return __longlong_as_double(old);\n",
        "}\n",
        "\n",
        "__device__\n",
        "double sumBlock(thread_group g, double *dotp_temp2, double val){\n",
        "  int thread_index = g.thread_rank();\n",
        "\n",
        "  for (int i = blockDim.x / 2; i > 0; i /= 2) {\n",
        "    dotp_temp2[thread_index] = val;\n",
        "    g.sync();\n",
        "    if(thread_index<i){\n",
        "      val += dotp_temp2[thread_index + i];\n",
        "    }\n",
        "    g.sync();\n",
        "  }\n",
        "\n",
        "  return val;\n",
        "}\n",
        "\n",
        "__global__\n",
        "void sumDot(int n, int numThreads, float* X, float* Y, double* dotp, double* dotp_temp, double* dotp_temp2){\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "  thread_block g = this_thread_block();\n",
        "  g.sync();\n",
        "  double block_sum = sumBlock(g, dotp_temp2, dotp_temp[index]);\n",
        "  g.sync();\n",
        "\n",
        "  if (threadIdx.x == 0){\n",
        "    atomicAddDouble(dotp, block_sum);\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__\n",
        "void dotProduct(int n, int numThreads, float* X, float* Y, double* dotp, double* dotp_temp, double* dotp_temp2){\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "\n",
        "  // Fill dotp_temp with the product\n",
        "  for(int i = index; i < n; i += stride){\n",
        "    dotp_temp[i] = X[i] * Y[i];\n",
        "  }\n",
        "}\n",
        "\n",
        "int bulk_logic(unsigned int array_size){\n",
        "  const unsigned int ARRAY_SIZE = array_size;\n",
        "  const unsigned int ARRAY_BYTES = ARRAY_SIZE * sizeof(float);\n",
        "\n",
        "  //declare array\n",
        "  float *X, *Y;\n",
        "  double *dotp, *dotp_temp, *dotp_temp2;\n",
        "  cudaMallocManaged(&X, ARRAY_BYTES);\n",
        "  cudaMallocManaged(&Y, ARRAY_BYTES);\n",
        "  cudaMallocManaged(&dotp, ARRAY_SIZE * sizeof(double));\n",
        "  cudaMallocManaged(&dotp_temp, ARRAY_SIZE * sizeof(double));\n",
        "  cudaMallocManaged(&dotp_temp2, ARRAY_SIZE * sizeof(double));\n",
        "\n",
        "  //prefetch data to create CPU page memory\n",
        "  int device = -1;\n",
        "  cudaGetDevice(&device);\n",
        "\n",
        "  //memory advise\n",
        "  cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "  cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId);\n",
        "\n",
        "  //\"prefetch data\" to create CPU page memory\n",
        "  cudaMemPrefetchAsync(X, ARRAY_BYTES, cudaCpuDeviceId, NULL);\n",
        "  cudaMemPrefetchAsync(Y, ARRAY_BYTES, cudaCpuDeviceId, NULL);\n",
        "\n",
        "  //\"prefetch data\" to create GPU page memory\n",
        "  cudaMemPrefetchAsync(dotp, ARRAY_SIZE * sizeof(double), device, NULL);\n",
        "  cudaMemPrefetchAsync(dotp_temp, ARRAY_SIZE * sizeof(double), device, NULL);\n",
        "  cudaMemPrefetchAsync(dotp_temp2, ARRAY_SIZE * sizeof(double), device, NULL);\n",
        "\n",
        "  //initialize array\n",
        "  for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "    X[i] = float(1);\n",
        "    Y[i] = float(1);\n",
        "  }\n",
        "\n",
        "  //(to transfer data from the) Actual transfer: CPU to GPU\n",
        "  cudaMemPrefetchAsync(X, ARRAY_BYTES, device, NULL);\n",
        "  cudaMemPrefetchAsync(Y, ARRAY_BYTES, device, NULL);\n",
        "\n",
        "  //call cuda kernel\n",
        "  int numThreads = 1024;\n",
        "  int numBlocks = (ARRAY_SIZE+numThreads-1)/numThreads;\n",
        "  printf(\"numBlocks = %d, numThreads = %d\\n\", numBlocks, numThreads);\n",
        "\n",
        "  dotProduct<<<numBlocks, numThreads>>> (ARRAY_SIZE, numThreads, X, Y, dotp, dotp_temp, dotp_temp2);\n",
        "  sumDot<<<numBlocks, numThreads>>> (ARRAY_SIZE, numThreads, X, Y, dotp, dotp_temp, dotp_temp2);\n",
        "\n",
        "  //barrier\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  //memory advise\n",
        "  cudaMemAdvise(dotp, ARRAY_SIZE * sizeof(double), cudaMemAdviseSetReadMostly, device);\n",
        "  cudaMemPrefetchAsync(dotp, ARRAY_SIZE * sizeof(double), cudaCpuDeviceId, NULL);\n",
        "\n",
        "  //error checking\n",
        "  double expected_dotp  =  0.0;\n",
        "  for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "    expected_dotp = expected_dotp + (X[i] * Y[i]);\n",
        "  }\n",
        "\n",
        "  printf(\"Actual Dot Product: %.2f\\n\", *dotp);\n",
        "  printf(\"Expected Dot Product: %.2f\\n\", expected_dotp);\n",
        "\n",
        "  if(*dotp != expected_dotp){\n",
        "    printf(\"ERROR!: Actual Dot Product and Expected Dot Product is NOT EQUAL\\n\");\n",
        "  }\n",
        "  else{\n",
        "    printf(\"NO ERROR!\\n\");\n",
        "  }\n",
        "\n",
        "  //free memory\n",
        "  cudaFree(X);\n",
        "  cudaFree(Y);\n",
        "  cudaFree(dotp);\n",
        "  cudaFree(dotp_temp);\n",
        "  cudaFree(dotp_temp2);\n",
        "\n",
        "  return 0;\n",
        "}\n",
        "\n",
        "int main(){\n",
        "  int numberOfRuns = 30;\n",
        "  //############CHANGE HERE######################\n",
        "  unsigned int array_size = 1<<24;\n",
        "  //############CHANGE HERE######################\n",
        "\n",
        "  for (int i = 0; i < numberOfRuns; i++) {\n",
        "    bulk_logic(array_size);\n",
        "  }\n",
        "return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4F8UTASi-KG",
        "outputId": "a00759c4-4a59-4dd9-baf2-d63a72a65536"
      },
      "outputs": [
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "nvcc cuda_dotproduct2.cu -o cuda_dotproduct2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cboPMUI5jDLU",
        "outputId": "3f764f4e-6348-4d2c-f399-45002f3aaed3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==24973== NVPROF is profiling process 24973, command: ./cuda_dotproduct2\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "numBlocks = 16384, numThreads = 1024\n",
            "Actual Dot Product: 16777216.00\n",
            "Expected Dot Product: 16777216.00\n",
            "NO ERROR!\n",
            "==24973== Profiling application: ./cuda_dotproduct2\n",
            "==24973== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   85.39%  240.14ms        30  8.0047ms  8.0038ms  8.0060ms  sumDot(int, int, float*, float*, double*, double*, double*)\n",
            "                   14.61%  41.071ms        30  1.3690ms  1.3675ms  1.3708ms  dotProduct(int, int, float*, float*, double*, double*, double*)\n",
            "      API calls:   62.58%  1.97963s       240  8.2484ms  4.4130us  22.821ms  cudaMemPrefetchAsync\n",
            "                   20.53%  649.48ms       150  4.3299ms  867.39us  11.519ms  cudaFree\n",
            "                    8.89%  281.10ms        30  9.3698ms  9.3516ms  9.3774ms  cudaDeviceSynchronize\n",
            "                    7.78%  246.26ms       150  1.6418ms  11.839us  243.36ms  cudaMallocManaged\n",
            "                    0.15%  4.8577ms       150  32.384us  1.0450us  212.22us  cudaMemAdvise\n",
            "                    0.06%  1.7851ms        60  29.750us  7.7030us  65.491us  cudaLaunchKernel\n",
            "                    0.00%  124.65us       101  1.2340us     127ns  52.045us  cuDeviceGetAttribute\n",
            "                    0.00%  72.439us        30  2.4140us  2.0780us  2.8660us  cudaGetDevice\n",
            "                    0.00%  27.402us         1  27.402us  27.402us  27.402us  cuDeviceGetName\n",
            "                    0.00%  8.2150us         1  8.2150us  8.2150us  8.2150us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.0690us         3     689ns     174ns  1.5190us  cuDeviceGetCount\n",
            "                    0.00%  1.1480us         2     574ns     254ns     894ns  cuDeviceGet\n",
            "                    0.00%     582ns         1     582ns     582ns     582ns  cuDeviceTotalMem\n",
            "                    0.00%     437ns         1     437ns     437ns     437ns  cuModuleGetLoadingMode\n",
            "                    0.00%     245ns         1     245ns     245ns     245ns  cuDeviceGetUuid\n",
            "\n",
            "==24973== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "    1920  2.0000MB  2.0000MB  2.0000MB  3.750000GB  339.5429ms  Host To Device\n",
            "    1920  2.0000MB  2.0000MB  2.0000MB  3.750000GB  309.2277ms  Device To Host\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "nvprof ./cuda_dotproduct2"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "BduQmJubPofx",
        "kGHxqqQsUAE9"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
